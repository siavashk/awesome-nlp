{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a4bc8a-9499-40e8-ac9c-6eca0f26adb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:44:34.500568Z",
     "iopub.status.busy": "2025-10-26T23:44:34.500297Z",
     "iopub.status.idle": "2025-10-26T23:44:50.434254Z",
     "shell.execute_reply": "2025-10-26T23:44:50.433474Z",
     "shell.execute_reply.started": "2025-10-26T23:44:34.500546Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import sentencepiece as spm\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1dbde3-a50c-4942-99ca-f804aad6b794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:44:54.337265Z",
     "iopub.status.busy": "2025-10-26T23:44:54.336767Z",
     "iopub.status.idle": "2025-10-26T23:44:54.637447Z",
     "shell.execute_reply": "2025-10-26T23:44:54.636561Z",
     "shell.execute_reply.started": "2025-10-26T23:44:54.337241Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opus_books (/home/sagemaker-user/.cache/huggingface/datasets/opus_books/de-en/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5221022349e14d53a83be35faf7af220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"opus_books\", \"de-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ecf337-7224-4074-9077-f88fc239d089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:44:57.732883Z",
     "iopub.status.busy": "2025-10-26T23:44:57.732465Z",
     "iopub.status.idle": "2025-10-26T23:45:04.691992Z",
     "shell.execute_reply": "2025-10-26T23:45:04.691062Z",
     "shell.execute_reply.started": "2025-10-26T23:44:57.732857Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/seq2seq_nmt\")\n",
    "sp_dir = data_dir / \"spm\"\n",
    "sp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dump raw text to files SentencePiece can read\n",
    "def dump_corpus(split, lang, out_path):\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds[split]:\n",
    "            f.write(ex[\"translation\"][lang].strip() + \"\\n\")\n",
    "\n",
    "dump_corpus(\"train\", \"en\", sp_dir / \"train.en.txt\")\n",
    "dump_corpus(\"train\", \"de\", sp_dir / \"train.de.txt\")\n",
    "\n",
    "# Train two SPM models (unigram or bpe; paper used words, but subwords are practical)\n",
    "VOCAB_EN = 8000\n",
    "VOCAB_DE = 8000\n",
    "SPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\"]  # idx 0..2 (we'll enforce)\n",
    "\n",
    "def train_spm(input_txt: Path, model_prefix: str, vocab_size: int):\n",
    "    cmd = (\n",
    "        f\"--input={input_txt} --model_prefix={model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size - len(SPECIALS)} --character_coverage=1.0 \"\n",
    "        f\"--pad_id=0 --pad_piece=<pad> \"\n",
    "        f\"--bos_id=1 --bos_piece=<bos> \"\n",
    "        f\"--eos_id=2 --eos_piece=<eos> \"\n",
    "        f\"--unk_id=3 --model_type=unigram\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "if not (sp_dir / \"en.model\").exists():\n",
    "    train_spm(sp_dir / \"train.en.txt\", str(sp_dir / \"en\"), VOCAB_EN)\n",
    "if not (sp_dir / \"de.model\").exists():\n",
    "    train_spm(sp_dir / \"train.de.txt\", str(sp_dir / \"de\"), VOCAB_DE)\n",
    "\n",
    "sp_en = spm.SentencePieceProcessor(model_file=str(sp_dir / \"en.model\"))\n",
    "sp_de = spm.SentencePieceProcessor(model_file=str(sp_dir / \"de.model\"))\n",
    "\n",
    "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
    "VOCAB_EN = sp_en.get_piece_size()\n",
    "VOCAB_DE = sp_de.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d440c1-0ccb-4259-a004-75926391b430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:45:18.722597Z",
     "iopub.status.busy": "2025-10-26T23:45:18.722322Z",
     "iopub.status.idle": "2025-10-26T23:45:18.743673Z",
     "shell.execute_reply": "2025-10-26T23:45:18.742596Z",
     "shell.execute_reply.started": "2025-10-26T23:45:18.722576Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/sagemaker-user/.cache/huggingface/datasets/opus_books/de-en/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf/cache-9cca3c33736147e2.arrow and /home/sagemaker-user/.cache/huggingface/datasets/opus_books/de-en/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf/cache-431116ea61b76cb1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train']\n",
      "Now have splits: ['train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "SRC_LANG = \"en\"   # source language (encoder input)\n",
    "TGT_LANG = \"de\"   # target language (decoder output)\n",
    "\n",
    "print(\"Available splits:\", list(ds.keys()))\n",
    "\n",
    "# If \"validation\" is missing, carve it out of train\n",
    "if \"validation\" not in ds:\n",
    "    split = ds[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "    ds = DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"], **({} if \"test\" not in ds else {\"test\": ds[\"test\"]})})\n",
    "\n",
    "print(\"Now have splits:\", list(ds.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9302adad-8d15-48e9-9179-16a267c8ee50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:45:20.991641Z",
     "iopub.status.busy": "2025-10-26T23:45:20.991376Z",
     "iopub.status.idle": "2025-10-26T23:45:28.588991Z",
     "shell.execute_reply": "2025-10-26T23:45:28.587866Z",
     "shell.execute_reply.started": "2025-10-26T23:45:20.991623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46214, 2438)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    src_ids: list\n",
    "    src_len: int\n",
    "    tgt_in: list\n",
    "    tgt_out: list\n",
    "    tgt_len: int\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, split: str, reverse_source: bool = True, max_len: int = 100):\n",
    "        self.data = []\n",
    "        self.reverse_source = reverse_source\n",
    "        self.max_len = max_len\n",
    "        for ex in ds[split]:\n",
    "            src = ex[\"translation\"][\"en\"].strip()\n",
    "            tgt = ex[\"translation\"][\"de\"].strip()\n",
    "\n",
    "            src_ids = sp_en.encode(src, out_type=int)\n",
    "            tgt_ids = sp_de.encode(tgt, out_type=int)\n",
    "\n",
    "            if len(src_ids) == 0 or len(tgt_ids) == 0:\n",
    "                continue\n",
    "            if len(src_ids) > max_len or len(tgt_ids) > max_len:\n",
    "                continue\n",
    "\n",
    "            if reverse_source:\n",
    "                src_ids = list(reversed(src_ids))\n",
    "\n",
    "            # decoder inputs/outputs\n",
    "            tgt_in  = [BOS] + tgt_ids\n",
    "            tgt_out = tgt_ids + [EOS]\n",
    "\n",
    "            self.data.append(Example(src_ids, len(src_ids), tgt_in, tgt_out, len(tgt_out)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        e = self.data[i]\n",
    "        return e\n",
    "\n",
    "def collate(batch):\n",
    "    # pad to batch max\n",
    "    src_max = max(e.src_len for e in batch)\n",
    "    tgt_max = max(e.tgt_len for e in batch)  # for tgt_in/out they share length\n",
    "\n",
    "    bs = len(batch)\n",
    "    src = torch.full((bs, src_max), PAD, dtype=torch.long)\n",
    "    src_lens = torch.tensor([e.src_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    tgt_in = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_out = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_lens = torch.tensor([e.tgt_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    for i, e in enumerate(batch):\n",
    "        src[i, :e.src_len] = torch.tensor(e.src_ids)\n",
    "        tgt_in[i, :len(e.tgt_in)] = torch.tensor(e.tgt_in)\n",
    "        tgt_out[i, :len(e.tgt_out)] = torch.tensor(e.tgt_out)\n",
    "\n",
    "    return {\n",
    "        \"src\": src, \"src_lens\": src_lens,\n",
    "        \"tgt_in\": tgt_in, \"tgt_out\": tgt_out, \"tgt_lens\": tgt_lens\n",
    "    }\n",
    "\n",
    "train_ds = MTDataset(\"train\", reverse_source=True, max_len=80)\n",
    "valid_ds = MTDataset(\"validation\", reverse_source=True, max_len=80)\n",
    "\n",
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06aed426-4277-4165-b9a7-76dac75262ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:45:28.590500Z",
     "iopub.status.busy": "2025-10-26T23:45:28.590158Z",
     "iopub.status.idle": "2025-10-26T23:45:28.603614Z",
     "shell.execute_reply": "2025-10-26T23:45:28.602658Z",
     "shell.execute_reply.started": "2025-10-26T23:45:28.590469Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "    def forward(self, src, src_lens):\n",
    "        emb = self.embed(src)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h, c) = self.lstm(packed)\n",
    "        return h, c  # [L, B, H]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.proj = nn.Linear(hidden, vocab_size, bias=False)\n",
    "        if tie_weights:\n",
    "            assert emb_dim == hidden\n",
    "            self.proj.weight = self.embed.weight\n",
    "\n",
    "    def forward(self, tgt_in, h0, c0):\n",
    "        emb = self.embed(tgt_in)         # [B,T,E]\n",
    "        out, (h, c) = self.lstm(emb, (h0, c0))\n",
    "        logits = self.proj(out)          # [B,T,V]\n",
    "        return logits, (h, c)\n",
    "\n",
    "class Seq2SeqLM(pl.LightningModule):\n",
    "    def __init__(self, src_vocab, tgt_vocab, emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "                 lr=3e-4, label_smoothing=0.1, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(src_vocab, emb_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden, num_layers, dropout, tie_weights)\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        h0, c0 = self.encoder(batch[\"src\"], batch[\"src_lens\"])\n",
    "        logits, _ = self.decoder(batch[\"tgt_in\"], h0, c0)\n",
    "        return logits\n",
    "\n",
    "    def _step(self, batch, stage):\n",
    "        logits = self(batch)\n",
    "        B, T, V = logits.shape\n",
    "        loss = self.crit(logits.view(B*T, V), batch[\"tgt_out\"].view(B*T))\n",
    "        self.log(f\"{stage}/loss\", loss, prog_bar=(stage==\"train\"), on_epoch=True, on_step=(stage==\"train\"))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, idx):   return self._step(batch, \"train\")\n",
    "    def validation_step(self, batch, idx): return self._step(batch, \"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src_ids, src_len, max_len=80):\n",
    "        self.eval()\n",
    "        h, c = self.encoder(src_ids.unsqueeze(0), src_len.unsqueeze(0))\n",
    "        y = torch.tensor([[BOS]], device=self.device)\n",
    "        outs = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (h, c) = self.decoder(y, h, c)\n",
    "            nxt = logits[:, -1].softmax(-1).argmax(-1)\n",
    "            if nxt.item() == EOS: break\n",
    "            outs.append(nxt.item())\n",
    "            y = torch.cat([y, nxt.unsqueeze(1)], dim=1)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "001c90c0-a786-41f8-8c97-7ec28dadfa9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:45:28.612490Z",
     "iopub.status.busy": "2025-10-26T23:45:28.612259Z",
     "iopub.status.idle": "2025-10-26T23:45:28.618324Z",
     "shell.execute_reply": "2025-10-26T23:45:28.617535Z",
     "shell.execute_reply.started": "2025-10-26T23:45:28.612472Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83891650-d249-4678-aa1f-22ef591ffac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T23:45:31.403386Z",
     "iopub.status.busy": "2025-10-26T23:45:31.403109Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "2025-10-26 23:45:33.165328: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 10.4 M | train\n",
      "1 | decoder | Decoder          | 14.5 M | train\n",
      "2 | crit    | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.564    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f375eab1881462fa10ed4d7d5d61278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14510b312de64d82a988b760294f4844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Seq2SeqLM(\n",
    "    src_vocab=VOCAB_EN,     # English SPM size\n",
    "    tgt_vocab=VOCAB_DE,     # German  SPM size\n",
    "    emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "    lr=3e-4, label_smoothing=0.1, tie_weights=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=8,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe36d3-b933-4669-a008-2c6232f611ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
