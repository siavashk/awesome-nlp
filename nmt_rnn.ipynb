{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a4bc8a-9499-40e8-ac9c-6eca0f26adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import sentencepiece as spm\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e1dbde3-a50c-4942-99ca-f804aad6b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b03fa7fface41b78a65ba9cc2518075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea22214d8e81474a9804d7db990f5d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de-en/train-00000-of-00001.parquet:   0%|          | 0.00/8.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e39214fd914febb57f6f99f7dc394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"opus_books\", \"de-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ecf337-7224-4074-9077-f88fc239d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=../data/seq2seq_nmt/spm/train.en.txt --model_prefix=../data/seq2seq_nmt/spm/en --vocab_size=7997 --character_coverage=1.0 --pad_id=0 --pad_piece=<pad> --bos_id=1 --bos_piece=<bos> --eos_id=2 --eos_piece=<eos> --unk_id=3 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/seq2seq_nmt/spm/train.en.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/seq2seq_nmt/spm/en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 7997\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <bos>\n",
      "  eos_piece: <eos>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ../data/seq2seq_nmt/spm/train.en.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 51467 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <bos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <eos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=5855976\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=129\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 51467 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3004235\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 93713 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 51467\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 77455\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 77455 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=32929 obj=10.6795 num_tokens=163254 num_tokens/piece=4.95776\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27894 obj=8.43682 num_tokens=163680 num_tokens/piece=5.86793\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20920 obj=8.42588 num_tokens=172875 num_tokens/piece=8.26362\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20909 obj=8.40869 num_tokens=172860 num_tokens/piece=8.26725\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15680 obj=8.52103 num_tokens=187183 num_tokens/piece=11.9377\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15680 obj=8.4969 num_tokens=187172 num_tokens/piece=11.937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11760 obj=8.65396 num_tokens=203591 num_tokens/piece=17.3122\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11760 obj=8.62411 num_tokens=203594 num_tokens/piece=17.3124\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8820 obj=8.82279 num_tokens=221739 num_tokens/piece=25.1405\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8820 obj=8.78703 num_tokens=221743 num_tokens/piece=25.1409\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8796 obj=8.78859 num_tokens=221973 num_tokens/piece=25.2357\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8796 obj=8.78826 num_tokens=221977 num_tokens/piece=25.2361\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ../data/seq2seq_nmt/spm/en.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ../data/seq2seq_nmt/spm/en.vocab\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=../data/seq2seq_nmt/spm/train.de.txt --model_prefix=../data/seq2seq_nmt/spm/de --vocab_size=7997 --character_coverage=1.0 --pad_id=0 --pad_piece=<pad> --bos_id=1 --bos_piece=<bos> --eos_id=2 --eos_piece=<eos> --unk_id=3 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/seq2seq_nmt/spm/train.de.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/seq2seq_nmt/spm/de\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 7997\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <bos>\n",
      "  eos_piece: <eos>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ../data/seq2seq_nmt/spm/train.de.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 51467 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <bos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <eos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=6954357\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=137\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 51467 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3552244\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 197305 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 51467\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 107283\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 107283 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=52883 obj=11.4938 num_tokens=209986 num_tokens/piece=3.97077\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45785 obj=8.99635 num_tokens=210838 num_tokens/piece=4.60496\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34320 obj=9.0182 num_tokens=224243 num_tokens/piece=6.53389\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34290 obj=8.9944 num_tokens=224235 num_tokens/piece=6.53937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25714 obj=9.13883 num_tokens=244104 num_tokens/piece=9.49304\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25710 obj=9.10613 num_tokens=244141 num_tokens/piece=9.49595\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19281 obj=9.29815 num_tokens=266244 num_tokens/piece=13.8086\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19280 obj=9.25994 num_tokens=266262 num_tokens/piece=13.8103\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14460 obj=9.48943 num_tokens=290748 num_tokens/piece=20.1071\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14460 obj=9.44877 num_tokens=290752 num_tokens/piece=20.1073\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10845 obj=9.7124 num_tokens=315369 num_tokens/piece=29.0797\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10845 obj=9.66699 num_tokens=315379 num_tokens/piece=29.0806\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8796 obj=9.87544 num_tokens=333284 num_tokens/piece=37.8904\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8796 obj=9.84103 num_tokens=333287 num_tokens/piece=37.8907\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ../data/seq2seq_nmt/spm/de.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ../data/seq2seq_nmt/spm/de.vocab\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../data/seq2seq_nmt\")\n",
    "sp_dir = data_dir / \"spm\"\n",
    "sp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dump raw text to files SentencePiece can read\n",
    "def dump_corpus(split, lang, out_path):\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds[split]:\n",
    "            f.write(ex[\"translation\"][lang].strip() + \"\\n\")\n",
    "\n",
    "dump_corpus(\"train\", \"en\", sp_dir / \"train.en.txt\")\n",
    "dump_corpus(\"train\", \"de\", sp_dir / \"train.de.txt\")\n",
    "\n",
    "# Train two SPM models (unigram or bpe; paper used words, but subwords are practical)\n",
    "VOCAB_EN = 8000\n",
    "VOCAB_DE = 8000\n",
    "SPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\"]  # idx 0..2 (we'll enforce)\n",
    "\n",
    "def train_spm(input_txt: Path, model_prefix: str, vocab_size: int):\n",
    "    cmd = (\n",
    "        f\"--input={input_txt} --model_prefix={model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size - len(SPECIALS)} --character_coverage=1.0 \"\n",
    "        f\"--pad_id=0 --pad_piece=<pad> \"\n",
    "        f\"--bos_id=1 --bos_piece=<bos> \"\n",
    "        f\"--eos_id=2 --eos_piece=<eos> \"\n",
    "        f\"--unk_id=3 --model_type=unigram\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "if not (sp_dir / \"en.model\").exists():\n",
    "    train_spm(sp_dir / \"train.en.txt\", str(sp_dir / \"en\"), VOCAB_EN)\n",
    "if not (sp_dir / \"de.model\").exists():\n",
    "    train_spm(sp_dir / \"train.de.txt\", str(sp_dir / \"de\"), VOCAB_DE)\n",
    "\n",
    "sp_en = spm.SentencePieceProcessor(model_file=str(sp_dir / \"en.model\"))\n",
    "sp_de = spm.SentencePieceProcessor(model_file=str(sp_dir / \"de.model\"))\n",
    "\n",
    "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
    "VOCAB_EN = sp_en.get_piece_size()\n",
    "VOCAB_DE = sp_de.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d440c1-0ccb-4259-a004-75926391b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train']\n",
      "Now have splits: ['train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "SRC_LANG = \"en\"   # source language (encoder input)\n",
    "TGT_LANG = \"de\"   # target language (decoder output)\n",
    "\n",
    "print(\"Available splits:\", list(ds.keys()))\n",
    "\n",
    "# If \"validation\" is missing, carve it out of train\n",
    "if \"validation\" not in ds:\n",
    "    split = ds[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "    ds = DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"], **({} if \"test\" not in ds else {\"test\": ds[\"test\"]})})\n",
    "\n",
    "print(\"Now have splits:\", list(ds.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9302adad-8d15-48e9-9179-16a267c8ee50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46214, 2438)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    src_ids: list\n",
    "    src_len: int\n",
    "    tgt_in: list\n",
    "    tgt_out: list\n",
    "    tgt_len: int\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, split: str, reverse_source: bool = True, max_len: int = 100):\n",
    "        self.data = []\n",
    "        self.reverse_source = reverse_source\n",
    "        self.max_len = max_len\n",
    "        for ex in ds[split]:\n",
    "            src = ex[\"translation\"][\"en\"].strip()\n",
    "            tgt = ex[\"translation\"][\"de\"].strip()\n",
    "\n",
    "            src_ids = sp_en.encode(src, out_type=int)\n",
    "            tgt_ids = sp_de.encode(tgt, out_type=int)\n",
    "\n",
    "            if len(src_ids) == 0 or len(tgt_ids) == 0:\n",
    "                continue\n",
    "            if len(src_ids) > max_len or len(tgt_ids) > max_len:\n",
    "                continue\n",
    "\n",
    "            if reverse_source:\n",
    "                src_ids = list(reversed(src_ids))\n",
    "\n",
    "            # decoder inputs/outputs\n",
    "            tgt_in  = [BOS] + tgt_ids\n",
    "            tgt_out = tgt_ids + [EOS]\n",
    "\n",
    "            self.data.append(Example(src_ids, len(src_ids), tgt_in, tgt_out, len(tgt_out)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        e = self.data[i]\n",
    "        return e\n",
    "\n",
    "def collate(batch):\n",
    "    # pad to batch max\n",
    "    src_max = max(e.src_len for e in batch)\n",
    "    tgt_max = max(e.tgt_len for e in batch)  # for tgt_in/out they share length\n",
    "\n",
    "    bs = len(batch)\n",
    "    src = torch.full((bs, src_max), PAD, dtype=torch.long)\n",
    "    src_lens = torch.tensor([e.src_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    tgt_in = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_out = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_lens = torch.tensor([e.tgt_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    for i, e in enumerate(batch):\n",
    "        src[i, :e.src_len] = torch.tensor(e.src_ids)\n",
    "        tgt_in[i, :len(e.tgt_in)] = torch.tensor(e.tgt_in)\n",
    "        tgt_out[i, :len(e.tgt_out)] = torch.tensor(e.tgt_out)\n",
    "\n",
    "    return {\n",
    "        \"src\": src, \"src_lens\": src_lens,\n",
    "        \"tgt_in\": tgt_in, \"tgt_out\": tgt_out, \"tgt_lens\": tgt_lens\n",
    "    }\n",
    "\n",
    "train_ds = MTDataset(\"train\", reverse_source=True, max_len=80)\n",
    "valid_ds = MTDataset(\"validation\", reverse_source=True, max_len=80)\n",
    "\n",
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06aed426-4277-4165-b9a7-76dac75262ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "    def forward(self, src, src_lens):\n",
    "        emb = self.embed(src)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h, c) = self.lstm(packed)\n",
    "        return h, c  # [L, B, H]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.proj = nn.Linear(hidden, vocab_size, bias=False)\n",
    "        if tie_weights:\n",
    "            assert emb_dim == hidden\n",
    "            self.proj.weight = self.embed.weight\n",
    "\n",
    "    def forward(self, tgt_in, h0, c0):\n",
    "        emb = self.embed(tgt_in)         # [B,T,E]\n",
    "        out, (h, c) = self.lstm(emb, (h0, c0))\n",
    "        logits = self.proj(out)          # [B,T,V]\n",
    "        return logits, (h, c)\n",
    "\n",
    "class Seq2SeqLM(pl.LightningModule):\n",
    "    def __init__(self, src_vocab, tgt_vocab, emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "                 lr=3e-4, label_smoothing=0.1, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(src_vocab, emb_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden, num_layers, dropout, tie_weights)\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        h0, c0 = self.encoder(batch[\"src\"], batch[\"src_lens\"])\n",
    "        logits, _ = self.decoder(batch[\"tgt_in\"], h0, c0)\n",
    "        return logits\n",
    "\n",
    "    def _step(self, batch, stage):\n",
    "        logits = self(batch)\n",
    "        B, T, V = logits.shape\n",
    "        loss = self.crit(logits.view(B*T, V), batch[\"tgt_out\"].view(B*T))\n",
    "        self.log(f\"{stage}/loss\", loss, prog_bar=(stage==\"train\"), on_epoch=True, on_step=(stage==\"train\"))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, idx):   return self._step(batch, \"train\")\n",
    "    def validation_step(self, batch, idx): return self._step(batch, \"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src_ids, src_len, max_len=80):\n",
    "        self.eval()\n",
    "        h, c = self.encoder(src_ids.unsqueeze(0), src_len.unsqueeze(0))\n",
    "        y = torch.tensor([[BOS]], device=self.device)\n",
    "        outs = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (h, c) = self.decoder(y, h, c)\n",
    "            nxt = logits[:, -1].softmax(-1).argmax(-1)\n",
    "            if nxt.item() == EOS: break\n",
    "            outs.append(nxt.item())\n",
    "            y = torch.cat([y, nxt.unsqueeze(1)], dim=1)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001c90c0-a786-41f8-8c97-7ec28dadfa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83891650-d249-4678-aa1f-22ef591ffac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 10.4 M | train\n",
      "1 | decoder | Decoder          | 14.5 M | train\n",
      "2 | crit    | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.564    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3f626e84cb4f93a6af32759c21116c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b2176d9e2840a186cd9e15c1260418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9111b903e934eaea04984ec316d1f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce19a647db242529bd2c0caddaa95aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9b749acccd41c79d11cf5292d37af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e9311ac4a54666a1cc48f1b2a27c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f585b21797d4935a7fe1a4a78f85693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc196bdf9d744def9dd24b4d7f49217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25ac6aacdf848af87902419497c11d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eba2489217f43f88981ad4cdbb25bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8545ce917a4a3aad4acfb9b2889a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c4e7fb391b4c20aa287051be3b889e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "model = Seq2SeqLM(\n",
    "    src_vocab=VOCAB_EN,     # English SPM size\n",
    "    tgt_vocab=VOCAB_DE,     # German  SPM size\n",
    "    emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "    lr=3e-4, label_smoothing=0.1, tie_weights=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ebe36d3-b933-4669-a008-2c6232f611ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detok_de(ids):\n",
    "    # strip at EOS if present\n",
    "    toks = [t for t in ids if t not in (PAD, BOS)]\n",
    "    if EOS in toks:\n",
    "        toks = toks[:toks.index(EOS)]\n",
    "    return sp_de.decode(toks)\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_examples(n=5):\n",
    "    model.eval()\n",
    "    for i in range(min(n, len(valid_ds))):\n",
    "        ex = valid_ds[i]\n",
    "        src_ids = torch.tensor(ex.src_ids, device=model.device)\n",
    "        src_len = torch.tensor(ex.src_len, device=model.device)\n",
    "        out_ids = model.greedy_decode(src_ids, src_len, max_len=80)\n",
    "        hyp = sp_de.decode(out_ids)\n",
    "\n",
    "        # gold\n",
    "        gold = detok_de(ex.tgt_out)\n",
    "        # remember: we reversed source for training; display the unreversed source\n",
    "        src_original = sp_en.decode(list(reversed(ex.src_ids)))\n",
    "        print(f\"SRC: {src_original}\\nHYP: {hyp}\\nREF: {gold}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a359a14f-a940-4b9c-9109-348eccd7fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: I turned, and Miss Ingram darted forwards from her sofa: the others, too, looked up from their several occupations; for at the same time a crunching of wheels and a splashing tramp of horse-hoofs became audible on the wet gravel.\n",
      "HYP: Ich erhob sich und blickte in die Höhe, die sich in der Kirche zu sehen schien.\n",
      "REF: Ich wandte mich um und sah, wie Miß Ingram mit der größten Eilfertigkeit von ihrem Sofa aufsprang. Auch die Übrigen blickten von ihren verschiedenen Beschäftigungen auf, denn im selben Augenblick wurde ein Knirschen von Rädern und platschende Huftritte draußen auf dem durchweichten Kieswege vor dem Hause hörbar.\n",
      "---\n",
      "SRC: I found that I could never let anyone else deal with this sort of work unless I wanted to harm both the client and the job I had taken on.\n",
      "HYP: Ich hatte nicht mehr zu sehen, als ob ich mich in der That zu verrühren.\n",
      "REF: Ich fand, daß ich diese Arbeit niemandem überlassen dürfe, wenn ich mich nicht an meinen Klienten und an der Aufgabe, die ich übernommen hatte, versündigen wollte.\n",
      "---\n",
      "SRC: \"Let her be taken care of; let her be treated as tenderly as may be: let her--\" he stopped and burst into tears.\n",
      "HYP: »Setzen Sie sich, sie zu machen!«\n",
      "REF: »Laß sie sorgsam behüten; laß sie so nachsichtig behandeln wie möglich, laß sie –« hier hielt er inne und brach in bittere Thränen aus.\n",
      "---\n",
      "SRC: The riding-habit decided her.\n",
      "HYP: Der Gesichtsausdruck war.\n",
      "REF: Das Reitkleid gab den Ausschlag.\n",
      "---\n",
      "SRC: I will announce you.'\n",
      "HYP: Ich werde Sie.«\n",
      "REF: Ich werde anmelden.«\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "translate_examples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef8470-aa21-4fa8-b5ca-ee09592e3780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
