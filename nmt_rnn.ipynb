{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a4bc8a-9499-40e8-ac9c-6eca0f26adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import sentencepiece as spm\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e1dbde3-a50c-4942-99ca-f804aad6b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b03fa7fface41b78a65ba9cc2518075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea22214d8e81474a9804d7db990f5d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de-en/train-00000-of-00001.parquet:   0%|          | 0.00/8.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e39214fd914febb57f6f99f7dc394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"opus_books\", \"de-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ecf337-7224-4074-9077-f88fc239d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=../data/seq2seq_nmt/spm/train.en.txt --model_prefix=../data/seq2seq_nmt/spm/en --vocab_size=7997 --character_coverage=1.0 --pad_id=0 --pad_piece=<pad> --bos_id=1 --bos_piece=<bos> --eos_id=2 --eos_piece=<eos> --unk_id=3 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/seq2seq_nmt/spm/train.en.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/seq2seq_nmt/spm/en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 7997\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <bos>\n",
      "  eos_piece: <eos>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ../data/seq2seq_nmt/spm/train.en.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 51467 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <bos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <eos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=5855976\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=129\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 51467 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3004235\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 93713 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 51467\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 77455\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 77455 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=32929 obj=10.6795 num_tokens=163254 num_tokens/piece=4.95776\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27894 obj=8.43682 num_tokens=163680 num_tokens/piece=5.86793\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20920 obj=8.42588 num_tokens=172875 num_tokens/piece=8.26362\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20909 obj=8.40869 num_tokens=172860 num_tokens/piece=8.26725\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15680 obj=8.52103 num_tokens=187183 num_tokens/piece=11.9377\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15680 obj=8.4969 num_tokens=187172 num_tokens/piece=11.937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11760 obj=8.65396 num_tokens=203591 num_tokens/piece=17.3122\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11760 obj=8.62411 num_tokens=203594 num_tokens/piece=17.3124\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8820 obj=8.82279 num_tokens=221739 num_tokens/piece=25.1405\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8820 obj=8.78703 num_tokens=221743 num_tokens/piece=25.1409\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8796 obj=8.78859 num_tokens=221973 num_tokens/piece=25.2357\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8796 obj=8.78826 num_tokens=221977 num_tokens/piece=25.2361\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ../data/seq2seq_nmt/spm/en.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ../data/seq2seq_nmt/spm/en.vocab\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=../data/seq2seq_nmt/spm/train.de.txt --model_prefix=../data/seq2seq_nmt/spm/de --vocab_size=7997 --character_coverage=1.0 --pad_id=0 --pad_piece=<pad> --bos_id=1 --bos_piece=<bos> --eos_id=2 --eos_piece=<eos> --unk_id=3 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/seq2seq_nmt/spm/train.de.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/seq2seq_nmt/spm/de\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 7997\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <bos>\n",
      "  eos_piece: <eos>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ../data/seq2seq_nmt/spm/train.de.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 51467 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <bos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <eos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=6954357\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=137\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 51467 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3552244\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 197305 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 51467\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 107283\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 107283 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=52883 obj=11.4938 num_tokens=209986 num_tokens/piece=3.97077\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45785 obj=8.99635 num_tokens=210838 num_tokens/piece=4.60496\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34320 obj=9.0182 num_tokens=224243 num_tokens/piece=6.53389\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34290 obj=8.9944 num_tokens=224235 num_tokens/piece=6.53937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25714 obj=9.13883 num_tokens=244104 num_tokens/piece=9.49304\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25710 obj=9.10613 num_tokens=244141 num_tokens/piece=9.49595\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19281 obj=9.29815 num_tokens=266244 num_tokens/piece=13.8086\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19280 obj=9.25994 num_tokens=266262 num_tokens/piece=13.8103\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14460 obj=9.48943 num_tokens=290748 num_tokens/piece=20.1071\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14460 obj=9.44877 num_tokens=290752 num_tokens/piece=20.1073\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10845 obj=9.7124 num_tokens=315369 num_tokens/piece=29.0797\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10845 obj=9.66699 num_tokens=315379 num_tokens/piece=29.0806\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8796 obj=9.87544 num_tokens=333284 num_tokens/piece=37.8904\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8796 obj=9.84103 num_tokens=333287 num_tokens/piece=37.8907\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ../data/seq2seq_nmt/spm/de.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ../data/seq2seq_nmt/spm/de.vocab\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../data/seq2seq_nmt\")\n",
    "sp_dir = data_dir / \"spm\"\n",
    "sp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dump raw text to files SentencePiece can read\n",
    "def dump_corpus(split, lang, out_path):\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds[split]:\n",
    "            f.write(ex[\"translation\"][lang].strip() + \"\\n\")\n",
    "\n",
    "dump_corpus(\"train\", \"en\", sp_dir / \"train.en.txt\")\n",
    "dump_corpus(\"train\", \"de\", sp_dir / \"train.de.txt\")\n",
    "\n",
    "# Train two SPM models (unigram or bpe; paper used words, but subwords are practical)\n",
    "VOCAB_EN = 8000\n",
    "VOCAB_DE = 8000\n",
    "SPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\"]  # idx 0..2 (we'll enforce)\n",
    "\n",
    "def train_spm(input_txt: Path, model_prefix: str, vocab_size: int):\n",
    "    cmd = (\n",
    "        f\"--input={input_txt} --model_prefix={model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size - len(SPECIALS)} --character_coverage=1.0 \"\n",
    "        f\"--pad_id=0 --pad_piece=<pad> \"\n",
    "        f\"--bos_id=1 --bos_piece=<bos> \"\n",
    "        f\"--eos_id=2 --eos_piece=<eos> \"\n",
    "        f\"--unk_id=3 --model_type=unigram\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "if not (sp_dir / \"en.model\").exists():\n",
    "    train_spm(sp_dir / \"train.en.txt\", str(sp_dir / \"en\"), VOCAB_EN)\n",
    "if not (sp_dir / \"de.model\").exists():\n",
    "    train_spm(sp_dir / \"train.de.txt\", str(sp_dir / \"de\"), VOCAB_DE)\n",
    "\n",
    "sp_en = spm.SentencePieceProcessor(model_file=str(sp_dir / \"en.model\"))\n",
    "sp_de = spm.SentencePieceProcessor(model_file=str(sp_dir / \"de.model\"))\n",
    "\n",
    "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
    "VOCAB_EN = sp_en.get_piece_size()\n",
    "VOCAB_DE = sp_de.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d440c1-0ccb-4259-a004-75926391b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train']\n",
      "Now have splits: ['train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "SRC_LANG = \"en\"   # source language (encoder input)\n",
    "TGT_LANG = \"de\"   # target language (decoder output)\n",
    "\n",
    "print(\"Available splits:\", list(ds.keys()))\n",
    "\n",
    "# If \"validation\" is missing, carve it out of train\n",
    "if \"validation\" not in ds:\n",
    "    split = ds[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "    ds = DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"], **({} if \"test\" not in ds else {\"test\": ds[\"test\"]})})\n",
    "\n",
    "print(\"Now have splits:\", list(ds.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9302adad-8d15-48e9-9179-16a267c8ee50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46214, 2438)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    src_ids: list\n",
    "    src_len: int\n",
    "    tgt_in: list\n",
    "    tgt_out: list\n",
    "    tgt_len: int\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, split: str, reverse_source: bool = True, max_len: int = 100):\n",
    "        self.data = []\n",
    "        self.reverse_source = reverse_source\n",
    "        self.max_len = max_len\n",
    "        for ex in ds[split]:\n",
    "            src = ex[\"translation\"][\"en\"].strip()\n",
    "            tgt = ex[\"translation\"][\"de\"].strip()\n",
    "\n",
    "            src_ids = sp_en.encode(src, out_type=int)\n",
    "            tgt_ids = sp_de.encode(tgt, out_type=int)\n",
    "\n",
    "            if len(src_ids) == 0 or len(tgt_ids) == 0:\n",
    "                continue\n",
    "            if len(src_ids) > max_len or len(tgt_ids) > max_len:\n",
    "                continue\n",
    "\n",
    "            if reverse_source:\n",
    "                src_ids = list(reversed(src_ids))\n",
    "\n",
    "            # decoder inputs/outputs\n",
    "            tgt_in  = [BOS] + tgt_ids\n",
    "            tgt_out = tgt_ids + [EOS]\n",
    "\n",
    "            self.data.append(Example(src_ids, len(src_ids), tgt_in, tgt_out, len(tgt_out)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        e = self.data[i]\n",
    "        return e\n",
    "\n",
    "def collate(batch):\n",
    "    # pad to batch max\n",
    "    src_max = max(e.src_len for e in batch)\n",
    "    tgt_max = max(e.tgt_len for e in batch)  # for tgt_in/out they share length\n",
    "\n",
    "    bs = len(batch)\n",
    "    src = torch.full((bs, src_max), PAD, dtype=torch.long)\n",
    "    src_lens = torch.tensor([e.src_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    tgt_in = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_out = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_lens = torch.tensor([e.tgt_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    for i, e in enumerate(batch):\n",
    "        src[i, :e.src_len] = torch.tensor(e.src_ids)\n",
    "        tgt_in[i, :len(e.tgt_in)] = torch.tensor(e.tgt_in)\n",
    "        tgt_out[i, :len(e.tgt_out)] = torch.tensor(e.tgt_out)\n",
    "\n",
    "    return {\n",
    "        \"src\": src, \"src_lens\": src_lens,\n",
    "        \"tgt_in\": tgt_in, \"tgt_out\": tgt_out, \"tgt_lens\": tgt_lens\n",
    "    }\n",
    "\n",
    "train_ds = MTDataset(\"train\", reverse_source=True, max_len=80)\n",
    "valid_ds = MTDataset(\"validation\", reverse_source=True, max_len=80)\n",
    "\n",
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06aed426-4277-4165-b9a7-76dac75262ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "    def forward(self, src, src_lens):\n",
    "        emb = self.embed(src)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h, c) = self.lstm(packed)\n",
    "        return h, c  # [L, B, H]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.proj = nn.Linear(hidden, vocab_size, bias=False)\n",
    "        if tie_weights:\n",
    "            assert emb_dim == hidden\n",
    "            self.proj.weight = self.embed.weight\n",
    "\n",
    "    def forward(self, tgt_in, h0, c0):\n",
    "        emb = self.embed(tgt_in)         # [B,T,E]\n",
    "        out, (h, c) = self.lstm(emb, (h0, c0))\n",
    "        logits = self.proj(out)          # [B,T,V]\n",
    "        return logits, (h, c)\n",
    "\n",
    "class Seq2SeqLM(pl.LightningModule):\n",
    "    def __init__(self, src_vocab, tgt_vocab, emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "                 lr=3e-4, label_smoothing=0.1, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(src_vocab, emb_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden, num_layers, dropout, tie_weights)\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        h0, c0 = self.encoder(batch[\"src\"], batch[\"src_lens\"])\n",
    "        logits, _ = self.decoder(batch[\"tgt_in\"], h0, c0)\n",
    "        return logits\n",
    "\n",
    "    def _step(self, batch, stage):\n",
    "        logits = self(batch)\n",
    "        B, T, V = logits.shape\n",
    "        loss = self.crit(logits.view(B*T, V), batch[\"tgt_out\"].view(B*T))\n",
    "        self.log(f\"{stage}/loss\", loss, prog_bar=(stage==\"train\"), on_epoch=True, on_step=(stage==\"train\"))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, idx):   return self._step(batch, \"train\")\n",
    "    def validation_step(self, batch, idx): return self._step(batch, \"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src_ids, src_len, max_len=80):\n",
    "        self.eval()\n",
    "        h, c = self.encoder(src_ids.unsqueeze(0), src_len.unsqueeze(0))\n",
    "        y = torch.tensor([[BOS]], device=self.device)\n",
    "        outs = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (h, c) = self.decoder(y, h, c)\n",
    "            nxt = logits[:, -1].softmax(-1).argmax(-1)\n",
    "            if nxt.item() == EOS: break\n",
    "            outs.append(nxt.item())\n",
    "            y = torch.cat([y, nxt.unsqueeze(1)], dim=1)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001c90c0-a786-41f8-8c97-7ec28dadfa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83891650-d249-4678-aa1f-22ef591ffac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 10.4 M | train\n",
      "1 | decoder | Decoder          | 14.5 M | train\n",
      "2 | crit    | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.564    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3f626e84cb4f93a6af32759c21116c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b2176d9e2840a186cd9e15c1260418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9111b903e934eaea04984ec316d1f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce19a647db242529bd2c0caddaa95aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9b749acccd41c79d11cf5292d37af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e9311ac4a54666a1cc48f1b2a27c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f585b21797d4935a7fe1a4a78f85693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc196bdf9d744def9dd24b4d7f49217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25ac6aacdf848af87902419497c11d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eba2489217f43f88981ad4cdbb25bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8545ce917a4a3aad4acfb9b2889a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c4e7fb391b4c20aa287051be3b889e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "model = Seq2SeqLM(\n",
    "    src_vocab=VOCAB_EN,     # English SPM size\n",
    "    tgt_vocab=VOCAB_DE,     # German  SPM size\n",
    "    emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "    lr=3e-4, label_smoothing=0.1, tie_weights=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ebe36d3-b933-4669-a008-2c6232f611ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detok_de(ids):\n",
    "    # strip at EOS if present\n",
    "    toks = [t for t in ids if t not in (PAD, BOS)]\n",
    "    if EOS in toks:\n",
    "        toks = toks[:toks.index(EOS)]\n",
    "    return sp_de.decode(toks)\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_examples(n=5):\n",
    "    model.eval()\n",
    "    for i in range(min(n, len(valid_ds))):\n",
    "        ex = valid_ds[i]\n",
    "        src_ids = torch.tensor(ex.src_ids, device=model.device)\n",
    "        src_len = torch.tensor(ex.src_len, device=model.device)\n",
    "        out_ids = model.greedy_decode(src_ids, src_len, max_len=80)\n",
    "        hyp = sp_de.decode(out_ids)\n",
    "\n",
    "        # gold\n",
    "        gold = detok_de(ex.tgt_out)\n",
    "        # remember: we reversed source for training; display the unreversed source\n",
    "        src_original = sp_en.decode(list(reversed(ex.src_ids)))\n",
    "        print(f\"SRC: {src_original}\\nHYP: {hyp}\\nREF: {gold}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a359a14f-a940-4b9c-9109-348eccd7fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: I turned, and Miss Ingram darted forwards from her sofa: the others, too, looked up from their several occupations; for at the same time a crunching of wheels and a splashing tramp of horse-hoofs became audible on the wet gravel.\n",
      "HYP: Ich erhob sich und blickte in die H√∂he, die sich in der Kirche zu sehen schien.\n",
      "REF: Ich wandte mich um und sah, wie Mi√ü Ingram mit der gr√∂√üten Eilfertigkeit von ihrem Sofa aufsprang. Auch die √úbrigen blickten von ihren verschiedenen Besch√§ftigungen auf, denn im selben Augenblick wurde ein Knirschen von R√§dern und platschende Huftritte drau√üen auf dem durchweichten Kieswege vor dem Hause h√∂rbar.\n",
      "---\n",
      "SRC: I found that I could never let anyone else deal with this sort of work unless I wanted to harm both the client and the job I had taken on.\n",
      "HYP: Ich hatte nicht mehr zu sehen, als ob ich mich in der That zu verr√ºhren.\n",
      "REF: Ich fand, da√ü ich diese Arbeit niemandem √ºberlassen d√ºrfe, wenn ich mich nicht an meinen Klienten und an der Aufgabe, die ich √ºbernommen hatte, vers√ºndigen wollte.\n",
      "---\n",
      "SRC: \"Let her be taken care of; let her be treated as tenderly as may be: let her--\" he stopped and burst into tears.\n",
      "HYP: ¬ªSetzen Sie sich, sie zu machen!¬´\n",
      "REF: ¬ªLa√ü sie sorgsam beh√ºten; la√ü sie so nachsichtig behandeln wie m√∂glich, la√ü sie ‚Äì¬´ hier hielt er inne und brach in bittere Thr√§nen aus.\n",
      "---\n",
      "SRC: The riding-habit decided her.\n",
      "HYP: Der Gesichtsausdruck war.\n",
      "REF: Das Reitkleid gab den Ausschlag.\n",
      "---\n",
      "SRC: I will announce you.'\n",
      "HYP: Ich werde Sie.¬´\n",
      "REF: Ich werde anmelden.¬´\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "translate_examples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef8470-aa21-4fa8-b5ca-ee09592e3780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
