{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a4bc8a-9499-40e8-ac9c-6eca0f26adb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:32:32.185740Z",
     "iopub.status.busy": "2025-10-27T05:32:32.185434Z",
     "iopub.status.idle": "2025-10-27T05:32:48.202960Z",
     "shell.execute_reply": "2025-10-27T05:32:48.201577Z",
     "shell.execute_reply.started": "2025-10-27T05:32:32.185716Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import sentencepiece as spm\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1dbde3-a50c-4942-99ca-f804aad6b794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:32:48.204806Z",
     "iopub.status.busy": "2025-10-27T05:32:48.204158Z",
     "iopub.status.idle": "2025-10-27T05:32:48.495961Z",
     "shell.execute_reply": "2025-10-27T05:32:48.495116Z",
     "shell.execute_reply.started": "2025-10-27T05:32:48.204776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opus_books (/home/sagemaker-user/.cache/huggingface/datasets/opus_books/de-en/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19adce1d47e46648b37558441b21d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"opus_books\", \"de-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ecf337-7224-4074-9077-f88fc239d089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:32:48.497825Z",
     "iopub.status.busy": "2025-10-27T05:32:48.497594Z",
     "iopub.status.idle": "2025-10-27T05:32:54.419853Z",
     "shell.execute_reply": "2025-10-27T05:32:54.418880Z",
     "shell.execute_reply.started": "2025-10-27T05:32:48.497806Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/seq2seq_nmt\")\n",
    "sp_dir = data_dir / \"spm\"\n",
    "sp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dump raw text to files SentencePiece can read\n",
    "def dump_corpus(split, lang, out_path):\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds[split]:\n",
    "            f.write(ex[\"translation\"][lang].strip() + \"\\n\")\n",
    "\n",
    "dump_corpus(\"train\", \"en\", sp_dir / \"train.en.txt\")\n",
    "dump_corpus(\"train\", \"de\", sp_dir / \"train.de.txt\")\n",
    "\n",
    "# Train two SPM models (unigram or bpe; paper used words, but subwords are practical)\n",
    "VOCAB_EN = 8000\n",
    "VOCAB_DE = 8000\n",
    "SPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\"]  # idx 0..2 (we'll enforce)\n",
    "\n",
    "def train_spm(input_txt: Path, model_prefix: str, vocab_size: int):\n",
    "    cmd = (\n",
    "        f\"--input={input_txt} --model_prefix={model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size - len(SPECIALS)} --character_coverage=1.0 \"\n",
    "        f\"--pad_id=0 --pad_piece=<pad> \"\n",
    "        f\"--bos_id=1 --bos_piece=<bos> \"\n",
    "        f\"--eos_id=2 --eos_piece=<eos> \"\n",
    "        f\"--unk_id=3 --model_type=unigram\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "if not (sp_dir / \"en.model\").exists():\n",
    "    train_spm(sp_dir / \"train.en.txt\", str(sp_dir / \"en\"), VOCAB_EN)\n",
    "if not (sp_dir / \"de.model\").exists():\n",
    "    train_spm(sp_dir / \"train.de.txt\", str(sp_dir / \"de\"), VOCAB_DE)\n",
    "\n",
    "sp_en = spm.SentencePieceProcessor(model_file=str(sp_dir / \"en.model\"))\n",
    "sp_de = spm.SentencePieceProcessor(model_file=str(sp_dir / \"de.model\"))\n",
    "\n",
    "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
    "VOCAB_EN = sp_en.get_piece_size()\n",
    "VOCAB_DE = sp_de.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d440c1-0ccb-4259-a004-75926391b430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:32:54.422436Z",
     "iopub.status.busy": "2025-10-27T05:32:54.421851Z",
     "iopub.status.idle": "2025-10-27T05:32:54.446121Z",
     "shell.execute_reply": "2025-10-27T05:32:54.445379Z",
     "shell.execute_reply.started": "2025-10-27T05:32:54.422401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/sagemaker-user/.cache/huggingface/datasets/opus_books/de-en/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf/cache-9cca3c33736147e2.arrow and /home/sagemaker-user/.cache/huggingface/datasets/opus_books/de-en/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf/cache-431116ea61b76cb1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train']\n",
      "Now have splits: ['train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "SRC_LANG = \"en\"   # source language (encoder input)\n",
    "TGT_LANG = \"de\"   # target language (decoder output)\n",
    "\n",
    "print(\"Available splits:\", list(ds.keys()))\n",
    "\n",
    "# If \"validation\" is missing, carve it out of train\n",
    "if \"validation\" not in ds:\n",
    "    split = ds[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "    ds = DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"], **({} if \"test\" not in ds else {\"test\": ds[\"test\"]})})\n",
    "\n",
    "print(\"Now have splits:\", list(ds.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9302adad-8d15-48e9-9179-16a267c8ee50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:32:54.447547Z",
     "iopub.status.busy": "2025-10-27T05:32:54.447091Z",
     "iopub.status.idle": "2025-10-27T05:33:03.379195Z",
     "shell.execute_reply": "2025-10-27T05:33:03.376318Z",
     "shell.execute_reply.started": "2025-10-27T05:32:54.447514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46214, 2438)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    src_ids: list\n",
    "    src_len: int\n",
    "    tgt_in: list\n",
    "    tgt_out: list\n",
    "    tgt_len: int\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, split: str, reverse_source: bool = True, max_len: int = 100):\n",
    "        self.data = []\n",
    "        self.reverse_source = reverse_source\n",
    "        self.max_len = max_len\n",
    "        for ex in ds[split]:\n",
    "            src = ex[\"translation\"][\"en\"].strip()\n",
    "            tgt = ex[\"translation\"][\"de\"].strip()\n",
    "\n",
    "            src_ids = sp_en.encode(src, out_type=int)\n",
    "            tgt_ids = sp_de.encode(tgt, out_type=int)\n",
    "\n",
    "            if len(src_ids) == 0 or len(tgt_ids) == 0:\n",
    "                continue\n",
    "            if len(src_ids) > max_len or len(tgt_ids) > max_len:\n",
    "                continue\n",
    "\n",
    "            if reverse_source:\n",
    "                src_ids = list(reversed(src_ids))\n",
    "\n",
    "            # decoder inputs/outputs\n",
    "            tgt_in  = [BOS] + tgt_ids\n",
    "            tgt_out = tgt_ids + [EOS]\n",
    "\n",
    "            self.data.append(Example(src_ids, len(src_ids), tgt_in, tgt_out, len(tgt_out)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        e = self.data[i]\n",
    "        return e\n",
    "\n",
    "def collate(batch):\n",
    "    # pad to batch max\n",
    "    src_max = max(e.src_len for e in batch)\n",
    "    tgt_max = max(e.tgt_len for e in batch)  # for tgt_in/out they share length\n",
    "\n",
    "    bs = len(batch)\n",
    "    src = torch.full((bs, src_max), PAD, dtype=torch.long)\n",
    "    src_lens = torch.tensor([e.src_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    tgt_in = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_out = torch.full((bs, tgt_max), PAD, dtype=torch.long)\n",
    "    tgt_lens = torch.tensor([e.tgt_len for e in batch], dtype=torch.long)\n",
    "\n",
    "    for i, e in enumerate(batch):\n",
    "        src[i, :e.src_len] = torch.tensor(e.src_ids)\n",
    "        tgt_in[i, :len(e.tgt_in)] = torch.tensor(e.tgt_in)\n",
    "        tgt_out[i, :len(e.tgt_out)] = torch.tensor(e.tgt_out)\n",
    "\n",
    "    return {\n",
    "        \"src\": src, \"src_lens\": src_lens,\n",
    "        \"tgt_in\": tgt_in, \"tgt_out\": tgt_out, \"tgt_lens\": tgt_lens\n",
    "    }\n",
    "\n",
    "train_ds = MTDataset(\"train\", reverse_source=True, max_len=80)\n",
    "valid_ds = MTDataset(\"validation\", reverse_source=True, max_len=80)\n",
    "\n",
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06aed426-4277-4165-b9a7-76dac75262ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:33:03.380351Z",
     "iopub.status.busy": "2025-10-27T05:33:03.380026Z",
     "iopub.status.idle": "2025-10-27T05:33:03.411259Z",
     "shell.execute_reply": "2025-10-27T05:33:03.410358Z",
     "shell.execute_reply.started": "2025-10-27T05:33:03.380327Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "    def forward(self, src, src_lens):\n",
    "        emb = self.embed(src)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h, c) = self.lstm(packed)\n",
    "        return h, c  # [L, B, H]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden=512, num_layers=3, dropout=0.2, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.proj = nn.Linear(hidden, vocab_size, bias=False)\n",
    "        if tie_weights:\n",
    "            assert emb_dim == hidden\n",
    "            self.proj.weight = self.embed.weight\n",
    "\n",
    "    def forward(self, tgt_in, h0, c0):\n",
    "        emb = self.embed(tgt_in)         # [B,T,E]\n",
    "        out, (h, c) = self.lstm(emb, (h0, c0))\n",
    "        logits = self.proj(out)          # [B,T,V]\n",
    "        return logits, (h, c)\n",
    "\n",
    "class Seq2SeqLM(pl.LightningModule):\n",
    "    def __init__(self, src_vocab, tgt_vocab, emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "                 lr=3e-4, label_smoothing=0.1, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(src_vocab, emb_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden, num_layers, dropout, tie_weights)\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        h0, c0 = self.encoder(batch[\"src\"], batch[\"src_lens\"])\n",
    "        logits, _ = self.decoder(batch[\"tgt_in\"], h0, c0)\n",
    "        return logits\n",
    "\n",
    "    def _step(self, batch, stage):\n",
    "        logits = self(batch)\n",
    "        B, T, V = logits.shape\n",
    "        loss = self.crit(logits.view(B*T, V), batch[\"tgt_out\"].view(B*T))\n",
    "        self.log(f\"{stage}/loss\", loss, prog_bar=(stage==\"train\"), on_epoch=True, on_step=(stage==\"train\"))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, idx):   return self._step(batch, \"train\")\n",
    "    def validation_step(self, batch, idx): return self._step(batch, \"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src_ids, src_len, max_len=80):\n",
    "        self.eval()\n",
    "        h, c = self.encoder(src_ids.unsqueeze(0), src_len.unsqueeze(0))\n",
    "        y = torch.tensor([[BOS]], device=self.device)\n",
    "        outs = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (h, c) = self.decoder(y, h, c)\n",
    "            nxt = logits[:, -1].softmax(-1).argmax(-1)\n",
    "            if nxt.item() == EOS: break\n",
    "            outs.append(nxt.item())\n",
    "            y = torch.cat([y, nxt.unsqueeze(1)], dim=1)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001c90c0-a786-41f8-8c97-7ec28dadfa9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:33:03.412760Z",
     "iopub.status.busy": "2025-10-27T05:33:03.412339Z",
     "iopub.status.idle": "2025-10-27T05:33:03.425188Z",
     "shell.execute_reply": "2025-10-27T05:33:03.423182Z",
     "shell.execute_reply.started": "2025-10-27T05:33:03.412729Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83891650-d249-4678-aa1f-22ef591ffac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T05:33:03.428341Z",
     "iopub.status.busy": "2025-10-27T05:33:03.427767Z",
     "iopub.status.idle": "2025-10-27T06:53:46.176680Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "2025-10-27 05:33:05.406914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 10.4 M | train\n",
      "1 | decoder | Decoder          | 14.5 M | train\n",
      "2 | crit    | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.564    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b008abd3224540a6fcfea3c70cb013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02cd715fee640b8883345f17158cd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f3e1317da645c0971712cadc9aa0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "model = Seq2SeqLM(\n",
    "    src_vocab=VOCAB_EN,     # English SPM size\n",
    "    tgt_vocab=VOCAB_DE,     # German  SPM size\n",
    "    emb_dim=512, hidden=512, num_layers=3, dropout=0.2,\n",
    "    lr=3e-4, label_smoothing=0.1, tie_weights=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ebe36d3-b933-4669-a008-2c6232f611ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T06:58:37.315262Z",
     "iopub.status.busy": "2025-10-27T06:58:37.314928Z",
     "iopub.status.idle": "2025-10-27T06:58:37.321247Z",
     "shell.execute_reply": "2025-10-27T06:58:37.320294Z",
     "shell.execute_reply.started": "2025-10-27T06:58:37.315234Z"
    }
   },
   "outputs": [],
   "source": [
    "def detok_de(ids):\n",
    "    # strip at EOS if present\n",
    "    toks = [t for t in ids if t not in (PAD, BOS)]\n",
    "    if EOS in toks:\n",
    "        toks = toks[:toks.index(EOS)]\n",
    "    return sp_de.decode(toks)\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_examples(n=5):\n",
    "    model.eval()\n",
    "    for i in range(min(n, len(valid_ds))):\n",
    "        ex = valid_ds[i]\n",
    "        src_ids = torch.tensor(ex.src_ids, device=model.device)\n",
    "        src_len = torch.tensor(ex.src_len, device=model.device)\n",
    "        out_ids = model.greedy_decode(src_ids, src_len, max_len=80)\n",
    "        hyp = sp_de.decode(out_ids)\n",
    "\n",
    "        # gold\n",
    "        gold = detok_de(ex.tgt_out)\n",
    "        # remember: we reversed source for training; display the unreversed source\n",
    "        src_original = sp_en.decode(list(reversed(ex.src_ids)))\n",
    "        print(f\"SRC: {src_original}\\nHYP: {hyp}\\nREF: {gold}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a359a14f-a940-4b9c-9109-348eccd7fe7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T06:58:38.758951Z",
     "iopub.status.busy": "2025-10-27T06:58:38.758658Z",
     "iopub.status.idle": "2025-10-27T06:58:39.601696Z",
     "shell.execute_reply": "2025-10-27T06:58:39.600885Z",
     "shell.execute_reply.started": "2025-10-27T06:58:38.758914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: I turned, and Miss Ingram darted forwards from her sofa: the others, too, looked up from their several occupations; for at the same time a crunching of wheels and a splashing tramp of horse-hoofs became audible on the wet gravel.\n",
      "HYP: Ich war, und er war sich nicht, daß er sich zu.\n",
      "REF: Ich wandte mich um und sah, wie Miß Ingram mit der größten Eilfertigkeit von ihrem Sofa aufsprang. Auch die Übrigen blickten von ihren verschiedenen Beschäftigungen auf, denn im selben Augenblick wurde ein Knirschen von Rädern und platschende Huftritte draußen auf dem durchweichten Kieswege vor dem Hause hörbar.\n",
      "---\n",
      "SRC: I found that I could never let anyone else deal with this sort of work unless I wanted to harm both the client and the job I had taken on.\n",
      "HYP: Ich war, daß sie nicht nicht zu.\n",
      "REF: Ich fand, daß ich diese Arbeit niemandem überlassen dürfe, wenn ich mich nicht an meinen Klienten und an der Aufgabe, die ich übernommen hatte, versündigen wollte.\n",
      "---\n",
      "SRC: \"Let her be taken care of; let her be treated as tenderly as may be: let her--\" he stopped and burst into tears.\n",
      "HYP: » ist die Frau, daß ich nicht zu.\n",
      "REF: »Laß sie sorgsam behüten; laß sie so nachsichtig behandeln wie möglich, laß sie –« hier hielt er inne und brach in bittere Thränen aus.\n",
      "---\n",
      "SRC: The riding-habit decided her.\n",
      "HYP: Die Frau.\n",
      "REF: Das Reitkleid gab den Ausschlag.\n",
      "---\n",
      "SRC: I will announce you.'\n",
      "HYP: Ich ist nicht.\n",
      "REF: Ich werde anmelden.«\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "translate_examples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef8470-aa21-4fa8-b5ca-ee09592e3780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
