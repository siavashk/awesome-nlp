{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a18f248d-1cf2-41fb-8c2e-55d26df310b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:42:56.049220Z",
     "iopub.status.busy": "2025-10-24T17:42:56.048969Z",
     "iopub.status.idle": "2025-10-24T17:42:56.052510Z",
     "shell.execute_reply": "2025-10-24T17:42:56.051936Z",
     "shell.execute_reply.started": "2025-10-24T17:42:56.049199Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7374d452-48c1-4f34-a1b6-118fb916e09c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:42:56.831794Z",
     "iopub.status.busy": "2025-10-24T17:42:56.831527Z",
     "iopub.status.idle": "2025-10-24T17:42:56.840088Z",
     "shell.execute_reply": "2025-10-24T17:42:56.839489Z",
     "shell.execute_reply.started": "2025-10-24T17:42:56.831773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text8 already extracted.\n",
      "Corpus path: /mnt/custom-file-systems/efs/fs-038956e0ab0e15389_fsap-093b86bb45d55576e/data/text8\n"
     ]
    }
   ],
   "source": [
    "# Download data and extract to ../data/text8/text8\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "data_dir = Path(\"../data\")\n",
    "zip_path = data_dir / \"text8.zip\"\n",
    "\n",
    "if not zip_path.exists():\n",
    "    print(\"Downloading text8.zip ...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "# Extract if needed\n",
    "extracted_file = data_dir / \"text8\"     # the corpus file (no extension)\n",
    "if not extracted_file.exists():\n",
    "    print(f\"Unzipping to {extract_dir} ...\")\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(extract_dir)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"text8 already extracted.\")\n",
    "\n",
    "print(\"Corpus path:\", extracted_file.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d66e91c-7fce-4395-a7c3-927e9f3f0b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:42:57.493623Z",
     "iopub.status.busy": "2025-10-24T17:42:57.493363Z",
     "iopub.status.idle": "2025-10-24T17:43:02.821681Z",
     "shell.execute_reply": "2025-10-24T17:43:02.821141Z",
     "shell.execute_reply.started": "2025-10-24T17:42:57.493604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw tokens: 17,005,207\n"
     ]
    }
   ],
   "source": [
    "# Convert corpus to tokens\n",
    "with open(extracted_file, \"r\") as handle:\n",
    "    corpus = handle.readline()\n",
    "\n",
    "tokens = corpus.lower().split()\n",
    "print(f\"Raw tokens: {len(tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77a033c-04e5-473d-9c92-b339d2486d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:43:02.822618Z",
     "iopub.status.busy": "2025-10-24T17:43:02.822406Z",
     "iopub.status.idle": "2025-10-24T17:43:05.421964Z",
     "shell.execute_reply": "2025-10-24T17:43:05.421428Z",
     "shell.execute_reply.started": "2025-10-24T17:43:02.822590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (min_count=5): 71,290\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary with minimum count cut-off\n",
    "min_count = 5\n",
    "freq = Counter(tokens)\n",
    "itos = [w for w, c in freq.items() if c >= min_count]\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "counts = np.array([freq[w] for w in itos], dtype=np.int64)\n",
    "print(f\"Vocab size (min_count={min_count}): {len(itos):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20fca5a-45a2-4975-9cb8-fcfcf8218f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:43:05.422665Z",
     "iopub.status.busy": "2025-10-24T17:43:05.422471Z",
     "iopub.status.idle": "2025-10-24T17:43:20.169522Z",
     "shell.execute_reply": "2025-10-24T17:43:20.168992Z",
     "shell.execute_reply.started": "2025-10-24T17:43:05.422648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After subsampling: 4,670,984 tokens\n"
     ]
    }
   ],
   "source": [
    "# Subsampling of frequent words (Mikolov et al.)\n",
    "# https://arxiv.org/pdf/1310.4546\n",
    "t = 1e-5\n",
    "total = counts.sum()\n",
    "freqs = counts / total\n",
    "p_keep = np.minimum(1.0, np.sqrt(t / freqs))\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "ids = []\n",
    "for w in tokens:\n",
    "    if w in stoi:\n",
    "        wid = stoi[w]\n",
    "        if rng.random() < p_keep[wid]:\n",
    "            ids.append(wid)\n",
    "\n",
    "ids = np.array(ids, dtype=np.int64)\n",
    "print(f\"After subsampling: {len(ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62edfab4-53f2-45eb-a371-3664653f9fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:43:20.170666Z",
     "iopub.status.busy": "2025-10-24T17:43:20.170458Z",
     "iopub.status.idle": "2025-10-24T17:43:20.175116Z",
     "shell.execute_reply": "2025-10-24T17:43:20.174592Z",
     "shell.execute_reply.started": "2025-10-24T17:43:20.170648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Negative sampling distribution (unigram^0.75)\n",
    "unigram = counts ** 0.75\n",
    "unigram = unigram / unigram.sum()\n",
    "cdf = np.cumsum(unigram)  # for fast sampling via inverse CDF\n",
    "\n",
    "\n",
    "def draw_negatives(k, forbidden_ids, rng):\n",
    "    \"\"\"Sample k negatives not in forbidden_ids.\"\"\"\n",
    "    out = []\n",
    "    while len(out) < k:\n",
    "        r = rng.random()\n",
    "        wid = int(np.searchsorted(cdf, r))\n",
    "        if wid not in forbidden_ids:\n",
    "            out.append(wid)\n",
    "    return np.array(out, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8901d431-4fd8-4c3b-8070-a6824d4f5810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:43:20.175959Z",
     "iopub.status.busy": "2025-10-24T17:43:20.175697Z",
     "iopub.status.idle": "2025-10-24T17:43:20.344265Z",
     "shell.execute_reply": "2025-10-24T17:43:20.343626Z",
     "shell.execute_reply.started": "2025-10-24T17:43:20.175934Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch Dataset: Skip-Gram with dynamic window + on-the-fly negatives\n",
    "class SkipGramNSDataset(Dataset):\n",
    "    def __init__(self, word_ids, window_max=5, num_negatives=5, seed=1234):\n",
    "        self.word_ids = np.asarray(word_ids, dtype=np.int64)\n",
    "        self.window_max = window_max\n",
    "        self.num_negatives = num_negatives\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Each index returns one (center, pos, negs) triple (random positive)\n",
    "        return len(self.word_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        center = self.word_ids[i]\n",
    "        R = int(self.rng.integers(1, self.window_max + 1))  # dynamic window\n",
    "        left = max(0, i - R)\n",
    "        right = min(len(self.word_ids), i + R + 1)\n",
    "\n",
    "        # Choose a positive context word at random from the window (excluding center)\n",
    "        window = self.word_ids[left:right]\n",
    "        if len(window) <= 1:\n",
    "            # Edge case (rare): no context; resample a nearby index\n",
    "            j = int(self.rng.integers(0, len(self.word_ids)))\n",
    "            return self.__getitem__(j)\n",
    "\n",
    "        # Exclude the center position i\n",
    "        # Compute the relative index within window\n",
    "        center_rel = i - left\n",
    "        candidates = np.delete(window, center_rel)\n",
    "        pos = int(self.rng.choice(candidates))\n",
    "\n",
    "        # Draw negative samples (avoid center & pos)\n",
    "        negs = draw_negatives(self.num_negatives, {center, pos}, self.rng)\n",
    "\n",
    "        return int(center), int(pos), negs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d021509b-5413-4266-84a8-add462b6b753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:43:20.345175Z",
     "iopub.status.busy": "2025-10-24T17:43:20.344950Z",
     "iopub.status.idle": "2025-10-24T17:43:20.348933Z",
     "shell.execute_reply": "2025-10-24T17:43:20.348425Z",
     "shell.execute_reply.started": "2025-10-24T17:43:20.345156Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    centers, positives, negatives = zip(*batch)\n",
    "    centers = torch.tensor(centers, dtype=torch.long)\n",
    "    positives = torch.tensor(positives, dtype=torch.long)\n",
    "    negatives = torch.tensor(np.stack(negatives), dtype=torch.long)  # [B, K]\n",
    "    return centers, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4009408-025e-442e-8806-0e86df8dae16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:02:50.802034Z",
     "iopub.status.busy": "2025-10-24T18:02:50.801773Z",
     "iopub.status.idle": "2025-10-24T18:02:50.805297Z",
     "shell.execute_reply": "2025-10-24T18:02:50.804688Z",
     "shell.execute_reply.started": "2025-10-24T18:02:50.802014Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = SkipGramNSDataset(ids, window_max=5, num_negatives=5, seed=2024)\n",
    "loader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=4,\n",
    "                    collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e539189d-1c18-4b68-80b1-cfa4ce9d8630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:02:53.728880Z",
     "iopub.status.busy": "2025-10-24T18:02:53.728590Z",
     "iopub.status.idle": "2025-10-24T18:02:54.714759Z",
     "shell.execute_reply": "2025-10-24T18:02:54.714046Z",
     "shell.execute_reply.started": "2025-10-24T18:02:53.728856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([1024]) torch.Size([1024]) torch.Size([1024, 5])\n",
      "Example: 8627 560 [439, 1294, 22165, 69327, 231]\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check\n",
    "centers, positives, negatives = next(iter(loader))\n",
    "print(\"Batch shapes:\", centers.shape, positives.shape, negatives.shape)\n",
    "print(\"Example:\", centers[0].item(), positives[0].item(), negatives[0][:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8565bca5-68c3-4485-9492-6d1fc431772a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:02:54.716270Z",
     "iopub.status.busy": "2025-10-24T18:02:54.715926Z",
     "iopub.status.idle": "2025-10-24T18:02:54.724928Z",
     "shell.execute_reply": "2025-10-24T18:02:54.724354Z",
     "shell.execute_reply.started": "2025-10-24T18:02:54.716243Z"
    }
   },
   "outputs": [],
   "source": [
    "# LightningModule for Skip-Gram + Negative Sampling\n",
    "class SkipGramNegativeSampling(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Skip-Gram with Negative Sampling (SGNS).\n",
    "    - Two embedding tables: input (for centers) and output (for contexts).\n",
    "    - Loss: -log Ïƒ(cÂ·p) - Î£_k log Ïƒ(-cÂ·n_k)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, dim: int = 300, lr: float = 2.5e-3, \n",
    "                 num_negatives: int = 5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        self.lr = lr\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "        # Input (center) embeddings and Output (context) embeddings\n",
    "        self.in_embed = nn.Embedding(vocab_size, dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # word2vec-style init: small uniform\n",
    "        bound = 0.5 / self.dim\n",
    "        nn.init.uniform_(self.in_embed.weight,  -bound, bound)\n",
    "        nn.init.zeros_(self.out_embed.weight)  # often initialized at 0 for out vectors\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embeddings(self, normalize: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Return the input embeddings (what you typically export).\"\"\"\n",
    "        E = self.in_embed.weight\n",
    "        if normalize:\n",
    "            E = F.normalize(E, dim=1)\n",
    "        return E\n",
    "\n",
    "    def forward(self, centers: torch.LongTensor, contexts: torch.LongTensor, \n",
    "                negatives: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        centers:   [B]\n",
    "        contexts:  [B]\n",
    "        negatives: [B, K]\n",
    "        Returns scores for positive and negative pairs.\n",
    "        \"\"\"\n",
    "        c = self.in_embed(centers)          # [B, D]\n",
    "        p = self.out_embed(contexts)        # [B, D]\n",
    "        n = self.out_embed(negatives)       # [B, K, D]\n",
    "\n",
    "        # Positive scores: dot(c, p)\n",
    "        pos_score = (c * p).sum(dim=-1)     # [B]\n",
    "\n",
    "        # Negative scores: dot(c, n_k) for each k\n",
    "        # Expand c to [B, 1, D] to broadcast across K negatives\n",
    "        neg_score = (n * c.unsqueeze(1)).sum(dim=-1)  # [B, K]\n",
    "\n",
    "        return pos_score, neg_score\n",
    "\n",
    "    def sgns_loss(self, pos_score, neg_score):\n",
    "        # -log Ïƒ(pos) - sum log Ïƒ(-neg)\n",
    "        loss_pos = F.logsigmoid(pos_score)              # [B]\n",
    "        loss_neg = F.logsigmoid(-neg_score).sum(dim=1)  # [B]\n",
    "        loss = -(loss_pos + loss_neg).mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        centers, positives, negatives = batch   # shapes: [B], [B], [B,K]\n",
    "        pos_score, neg_score = self(centers, positives, negatives)\n",
    "        loss = self.sgns_loss(pos_score, neg_score)\n",
    "\n",
    "        # (Optional) A couple of easy diagnostics to ensure things are sane\n",
    "        with torch.no_grad():\n",
    "            # Probability-like metrics just for logging intuition\n",
    "            pos_prob = torch.sigmoid(pos_score).mean()\n",
    "            neg_prob = torch.sigmoid(neg_score).mean()\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True,\n",
    "                 on_epoch=True)\n",
    "        self.log(\"train/pos_prob\", pos_prob, prog_bar=False, on_step=True,\n",
    "                 on_epoch=True)\n",
    "        self.log(\"train/neg_prob\", neg_prob, prog_bar=False, on_step=True,\n",
    "                 on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam is perfectly fine here (sparse updates are optional)\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d83b4be-5c7a-4b44-8872-3fd421b6e38d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:02:56.442830Z",
     "iopub.status.busy": "2025-10-24T18:02:56.442551Z",
     "iopub.status.idle": "2025-10-24T18:02:56.884117Z",
     "shell.execute_reply": "2025-10-24T18:02:56.883579Z",
     "shell.execute_reply.started": "2025-10-24T18:02:56.442809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(itos)\n",
    "model = SkipGramNegativeSampling(vocab_size=vocab_size, dim=300, lr=2.5e-3,\n",
    "                                 num_negatives=5)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=25,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"32-true\",   # \"16-mixed\" works too if you like\n",
    "    log_every_n_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e430e7c0-a9e8-4981-a02b-d29d36049fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:03:00.189195Z",
     "iopub.status.busy": "2025-10-24T18:03:00.188936Z",
     "iopub.status.idle": "2025-10-24T18:57:48.419263Z",
     "shell.execute_reply": "2025-10-24T18:57:48.418710Z",
     "shell.execute_reply.started": "2025-10-24T18:03:00.189175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | in_embed  | Embedding | 21.4 M | train\n",
      "1 | out_embed | Embedding | 21.4 M | train\n",
      "------------------------------------------------\n",
      "42.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "42.8 M    Total params\n",
      "171.096   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d179adec2d142acbf49a3b7eb4fba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db943c58-bcc7-4158-b8ce-2e38d8790927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:04:17.145727Z",
     "iopub.status.busy": "2025-10-24T19:04:17.145421Z",
     "iopub.status.idle": "2025-10-24T19:04:17.631794Z",
     "shell.execute_reply": "2025-10-24T19:04:17.631171Z",
     "shell.execute_reply.started": "2025-10-24T19:04:17.145697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export learned embeddings\n",
    "emb = model.get_embeddings(normalize=True).detach().cpu()  # [vocab_size, dim]\n",
    "torch.save({\"embeddings\": emb, \"itos\": itos}, \"../outputs/text8/sgns_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe0f9e47-e6a1-4498-9494-921f7d458db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:07:37.754956Z",
     "iopub.status.busy": "2025-10-24T19:07:37.754690Z",
     "iopub.status.idle": "2025-10-24T19:07:37.759048Z",
     "shell.execute_reply": "2025-10-24T19:07:37.758447Z",
     "shell.execute_reply.started": "2025-10-24T19:07:37.754934Z"
    }
   },
   "outputs": [],
   "source": [
    "# For projection visuali\n",
    "def save_tsv(emb, itos, out_dir=Path(\"../outputs/text8\"), stem=\"sgns\"):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    vec_path = out_dir / f\"{stem}.vec.tsv\"\n",
    "    tok_path = out_dir / f\"{stem}.tok.tsv\"\n",
    "    with open(vec_path, \"w\", encoding=\"utf-8\") as fv, open(tok_path, \"w\", encoding=\"utf-8\") as ft:\n",
    "        for i, w in enumerate(itos):\n",
    "            fv.write(\"\\t\".join(f\"{x:.6f}\" for x in emb[i].tolist()) + \"\\n\")\n",
    "            ft.write(w + \"\\n\")\n",
    "    return vec_path, tok_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac68c3aa-4c05-4ec4-81ea-29ca99831fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:06:46.627527Z",
     "iopub.status.busy": "2025-10-24T19:06:46.627263Z",
     "iopub.status.idle": "2025-10-24T19:06:46.648170Z",
     "shell.execute_reply": "2025-10-24T19:06:46.647502Z",
     "shell.execute_reply.started": "2025-10-24T19:06:46.627506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors('king'): ['prince', 'baladan', 'attalid', 'dubrawka', 'vi', 'monarchy', 'dingane', 'aeetes', 'monarch', 'anshan']\n",
      "neighbors('paris'): ['montparnasse', 'france', 'modernes', 'vienna', 'cimeti', 'mulhouse', 'roissy', 'quichotte', 'passy', 'lectronique']\n"
     ]
    }
   ],
   "source": [
    "# Quick nearest neighbors helper for sanity-checks\n",
    "@torch.no_grad()\n",
    "def nearest_neighbors(query_word: str, k: int = 10):\n",
    "    idx = stoi.get(query_word)\n",
    "    if idx is None:\n",
    "        return []\n",
    "    q = emb[idx].unsqueeze(0)               # [1, D]\n",
    "    sims = (emb @ q.T).squeeze(1)           # cosine if emb is normalized\n",
    "    topk = torch.topk(sims, k + 1).indices.tolist()  # include itself\n",
    "    return [itos[i] for i in topk if i != idx][:k]\n",
    "\n",
    "print(\"neighbors('king'):\", nearest_neighbors(\"king\")[:10])\n",
    "print(\"neighbors('paris'):\", nearest_neighbors(\"paris\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c35ee08-fea6-4f46-b186-a71f51915ef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:12:44.474497Z",
     "iopub.status.busy": "2025-10-24T19:12:44.474232Z",
     "iopub.status.idle": "2025-10-24T19:12:44.479334Z",
     "shell.execute_reply": "2025-10-24T19:12:44.478787Z",
     "shell.execute_reply.started": "2025-10-24T19:12:44.474476Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def analogy_3cosadd(a: str, b: str, c: str, topk: int = 10):\n",
    "    idx = [stoi.get(w) for w in (a, b, c)]\n",
    "    if any(i is None for i in idx):\n",
    "        missing = [w for w, i in zip((a,b,c), idx) if i is None]\n",
    "        raise ValueError(f\"OOV words: {missing}\")\n",
    "\n",
    "    va, vb, vc = emb[idx[0]], emb[idx[1]], emb[idx[2]]    # [D]\n",
    "    query = vb - va + vc                                   # [D]\n",
    "\n",
    "    # cosine similarity (emb is normalized; normalize query too)\n",
    "    query = torch.nn.functional.normalize(query.unsqueeze(0), dim=1)  # [1, D]\n",
    "    sims = (emb @ query.T).squeeze(1)                                  # [V]\n",
    "\n",
    "    # exclude the input words themselves\n",
    "    for i in idx:\n",
    "        sims[i] = -1e9\n",
    "\n",
    "    top_vals, top_ids = torch.topk(sims, topk)\n",
    "    return [(itos[i], float(v)) for i, v in zip(top_ids.tolist(), top_vals.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c33a4035-1a6b-470f-be27-c4431d547608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:13:20.153307Z",
     "iopub.status.busy": "2025-10-24T19:13:20.153039Z",
     "iopub.status.idle": "2025-10-24T19:13:20.162897Z",
     "shell.execute_reply": "2025-10-24T19:13:20.162295Z",
     "shell.execute_reply.started": "2025-10-24T19:13:20.153286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tyndareus', 0.31804510951042175)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_3cosadd(\"man\", \"king\", \"woman\", topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0cf9485-ff5d-45ee-82c4-7fd3aa3550bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:13:36.416402Z",
     "iopub.status.busy": "2025-10-24T19:13:36.416114Z",
     "iopub.status.idle": "2025-10-24T19:13:36.429659Z",
     "shell.execute_reply": "2025-10-24T19:13:36.429107Z",
     "shell.execute_reply.started": "2025-10-24T19:13:36.416379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('england', 0.29523101449012756)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_3cosadd(\"paris\", \"france\", \"london\", topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74302308-5585-4acd-9c8d-866bf80c3b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
