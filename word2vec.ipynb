{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18f248d-1cf2-41fb-8c2e-55d26df310b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:07:56.893744Z",
     "iopub.status.busy": "2025-10-24T06:07:56.893496Z",
     "iopub.status.idle": "2025-10-24T06:08:02.781345Z",
     "shell.execute_reply": "2025-10-24T06:08:02.780794Z",
     "shell.execute_reply.started": "2025-10-24T06:07:56.893724Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7374d452-48c1-4f34-a1b6-118fb916e09c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:04.002084Z",
     "iopub.status.busy": "2025-10-24T06:08:04.001651Z",
     "iopub.status.idle": "2025-10-24T06:08:04.012004Z",
     "shell.execute_reply": "2025-10-24T06:08:04.011497Z",
     "shell.execute_reply.started": "2025-10-24T06:08:04.002057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text8 already extracted.\n",
      "Corpus path: /mnt/custom-file-systems/efs/fs-038956e0ab0e15389_fsap-093b86bb45d55576e/data/text8\n"
     ]
    }
   ],
   "source": [
    "# Download data and extract to ../data/text8/text8\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "data_dir = Path(\"../data\")\n",
    "zip_path = data_dir / \"text8.zip\"\n",
    "\n",
    "if not zip_path.exists():\n",
    "    print(\"Downloading text8.zip ...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "# Extract if needed\n",
    "extracted_file = data_dir / \"text8\"     # the corpus file (no extension)\n",
    "if not extracted_file.exists():\n",
    "    print(f\"Unzipping to {extract_dir} ...\")\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(extract_dir)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"text8 already extracted.\")\n",
    "\n",
    "print(\"Corpus path:\", extracted_file.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d66e91c-7fce-4395-a7c3-927e9f3f0b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:04.513693Z",
     "iopub.status.busy": "2025-10-24T06:08:04.513433Z",
     "iopub.status.idle": "2025-10-24T06:08:09.699224Z",
     "shell.execute_reply": "2025-10-24T06:08:09.698648Z",
     "shell.execute_reply.started": "2025-10-24T06:08:04.513673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw tokens: 17,005,207\n"
     ]
    }
   ],
   "source": [
    "# Convert corpus to tokens\n",
    "with open(extracted_file, \"r\") as handle:\n",
    "    corpus = handle.readline()\n",
    "\n",
    "tokens = corpus.lower().split()\n",
    "print(f\"Raw tokens: {len(tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77a033c-04e5-473d-9c92-b339d2486d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:10.672742Z",
     "iopub.status.busy": "2025-10-24T06:08:10.672446Z",
     "iopub.status.idle": "2025-10-24T06:08:13.251285Z",
     "shell.execute_reply": "2025-10-24T06:08:13.250633Z",
     "shell.execute_reply.started": "2025-10-24T06:08:10.672721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (min_count=5): 71,290\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary with minimum count cut-off\n",
    "min_count = 5\n",
    "freq = Counter(tokens)\n",
    "itos = [w for w, c in freq.items() if c >= min_count]\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "counts = np.array([freq[w] for w in itos], dtype=np.int64)\n",
    "print(f\"Vocab size (min_count={min_count}): {len(itos):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20fca5a-45a2-4975-9cb8-fcfcf8218f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:13.252713Z",
     "iopub.status.busy": "2025-10-24T06:08:13.252383Z",
     "iopub.status.idle": "2025-10-24T06:08:27.919904Z",
     "shell.execute_reply": "2025-10-24T06:08:27.919305Z",
     "shell.execute_reply.started": "2025-10-24T06:08:13.252684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After subsampling: 4,670,984 tokens\n"
     ]
    }
   ],
   "source": [
    "# Subsampling of frequent words (Mikolov et al.)\n",
    "# https://arxiv.org/pdf/1310.4546\n",
    "t = 1e-5\n",
    "total = counts.sum()\n",
    "freqs = counts / total\n",
    "p_keep = np.minimum(1.0, np.sqrt(t / freqs))\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "ids = []\n",
    "for w in tokens:\n",
    "    if w in stoi:\n",
    "        wid = stoi[w]\n",
    "        if rng.random() < p_keep[wid]:\n",
    "            ids.append(wid)\n",
    "\n",
    "ids = np.array(ids, dtype=np.int64)\n",
    "print(f\"After subsampling: {len(ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62edfab4-53f2-45eb-a371-3664653f9fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:27.920713Z",
     "iopub.status.busy": "2025-10-24T06:08:27.920474Z",
     "iopub.status.idle": "2025-10-24T06:08:27.925131Z",
     "shell.execute_reply": "2025-10-24T06:08:27.924649Z",
     "shell.execute_reply.started": "2025-10-24T06:08:27.920695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Negative sampling distribution (unigram^0.75)\n",
    "unigram = counts ** 0.75\n",
    "unigram = unigram / unigram.sum()\n",
    "cdf = np.cumsum(unigram)  # for fast sampling via inverse CDF\n",
    "\n",
    "\n",
    "def draw_negatives(k, forbidden_ids, rng):\n",
    "    \"\"\"Sample k negatives not in forbidden_ids.\"\"\"\n",
    "    out = []\n",
    "    while len(out) < k:\n",
    "        r = rng.random()\n",
    "        wid = int(np.searchsorted(cdf, r))\n",
    "        if wid not in forbidden_ids:\n",
    "            out.append(wid)\n",
    "    return np.array(out, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8901d431-4fd8-4c3b-8070-a6824d4f5810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:27.926666Z",
     "iopub.status.busy": "2025-10-24T06:08:27.926378Z",
     "iopub.status.idle": "2025-10-24T06:08:27.934026Z",
     "shell.execute_reply": "2025-10-24T06:08:27.933508Z",
     "shell.execute_reply.started": "2025-10-24T06:08:27.926647Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch Dataset: Skip-Gram with dynamic window + on-the-fly negatives\n",
    "class SkipGramNSDataset(Dataset):\n",
    "    def __init__(self, word_ids, window_max=5, num_negatives=5, seed=1234):\n",
    "        self.word_ids = np.asarray(word_ids, dtype=np.int64)\n",
    "        self.window_max = window_max\n",
    "        self.num_negatives = num_negatives\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Each index returns one (center, pos, negs) triple (random positive)\n",
    "        return len(self.word_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        center = self.word_ids[i]\n",
    "        R = int(self.rng.integers(1, self.window_max + 1))  # dynamic window\n",
    "        left = max(0, i - R)\n",
    "        right = min(len(self.word_ids), i + R + 1)\n",
    "\n",
    "        # Choose a positive context word at random from the window (excluding center)\n",
    "        window = self.word_ids[left:right]\n",
    "        if len(window) <= 1:\n",
    "            # Edge case (rare): no context; resample a nearby index\n",
    "            j = int(self.rng.integers(0, len(self.word_ids)))\n",
    "            return self.__getitem__(j)\n",
    "\n",
    "        # Exclude the center position i\n",
    "        # Compute the relative index within window\n",
    "        center_rel = i - left\n",
    "        candidates = np.delete(window, center_rel)\n",
    "        pos = int(self.rng.choice(candidates))\n",
    "\n",
    "        # Draw negative samples (avoid center & pos)\n",
    "        negs = draw_negatives(self.num_negatives, {center, pos}, self.rng)\n",
    "\n",
    "        return int(center), int(pos), negs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d021509b-5413-4266-84a8-add462b6b753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:27.934857Z",
     "iopub.status.busy": "2025-10-24T06:08:27.934618Z",
     "iopub.status.idle": "2025-10-24T06:08:27.939626Z",
     "shell.execute_reply": "2025-10-24T06:08:27.939168Z",
     "shell.execute_reply.started": "2025-10-24T06:08:27.934838Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    centers, positives, negatives = zip(*batch)\n",
    "    centers = torch.tensor(centers, dtype=torch.long)\n",
    "    positives = torch.tensor(positives, dtype=torch.long)\n",
    "    negatives = torch.tensor(np.stack(negatives), dtype=torch.long)  # [B, K]\n",
    "    return centers, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4009408-025e-442e-8806-0e86df8dae16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:27.940366Z",
     "iopub.status.busy": "2025-10-24T06:08:27.940146Z",
     "iopub.status.idle": "2025-10-24T06:08:27.944439Z",
     "shell.execute_reply": "2025-10-24T06:08:27.943949Z",
     "shell.execute_reply.started": "2025-10-24T06:08:27.940319Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = SkipGramNSDataset(ids, window_max=5, num_negatives=5, seed=2024)\n",
    "loader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=0, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e539189d-1c18-4b68-80b1-cfa4ce9d8630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:27.945162Z",
     "iopub.status.busy": "2025-10-24T06:08:27.944979Z",
     "iopub.status.idle": "2025-10-24T06:08:28.468225Z",
     "shell.execute_reply": "2025-10-24T06:08:28.467651Z",
     "shell.execute_reply.started": "2025-10-24T06:08:27.945146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([1024]) torch.Size([1024]) torch.Size([1024, 5])\n",
      "Example: 2002 2912 [439, 1294, 22165, 69327, 231]\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check\n",
    "centers, positives, negatives = next(iter(loader))\n",
    "print(\"Batch shapes:\", centers.shape, positives.shape, negatives.shape)\n",
    "print(\"Example:\", centers[0].item(), positives[0].item(), negatives[0][:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8565bca5-68c3-4485-9492-6d1fc431772a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:28.469223Z",
     "iopub.status.busy": "2025-10-24T06:08:28.468940Z",
     "iopub.status.idle": "2025-10-24T06:08:28.477282Z",
     "shell.execute_reply": "2025-10-24T06:08:28.476773Z",
     "shell.execute_reply.started": "2025-10-24T06:08:28.469195Z"
    }
   },
   "outputs": [],
   "source": [
    "# LightningModule for Skip-Gram + Negative Sampling\n",
    "class SkipGramNegativeSampling(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Skip-Gram with Negative Sampling (SGNS).\n",
    "    - Two embedding tables: input (for centers) and output (for contexts).\n",
    "    - Loss: -log Ïƒ(cÂ·p) - Î£_k log Ïƒ(-cÂ·n_k)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, dim: int = 300, lr: float = 2.5e-3, \n",
    "                 num_negatives: int = 5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        self.lr = lr\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "        # Input (center) embeddings and Output (context) embeddings\n",
    "        self.in_embed = nn.Embedding(vocab_size, dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # word2vec-style init: small uniform\n",
    "        bound = 0.5 / self.dim\n",
    "        nn.init.uniform_(self.in_embed.weight,  -bound, bound)\n",
    "        nn.init.zeros_(self.out_embed.weight)  # often initialized at 0 for out vectors\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embeddings(self, normalize: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Return the input embeddings (what you typically export).\"\"\"\n",
    "        E = self.in_embed.weight\n",
    "        if normalize:\n",
    "            E = F.normalize(E, dim=1)\n",
    "        return E\n",
    "\n",
    "    def forward(self, centers: torch.LongTensor, contexts: torch.LongTensor, \n",
    "                negatives: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        centers:   [B]\n",
    "        contexts:  [B]\n",
    "        negatives: [B, K]\n",
    "        Returns scores for positive and negative pairs.\n",
    "        \"\"\"\n",
    "        c = self.in_embed(centers)          # [B, D]\n",
    "        p = self.out_embed(contexts)        # [B, D]\n",
    "        n = self.out_embed(negatives)       # [B, K, D]\n",
    "\n",
    "        # Positive scores: dot(c, p)\n",
    "        pos_score = (c * p).sum(dim=-1)     # [B]\n",
    "\n",
    "        # Negative scores: dot(c, n_k) for each k\n",
    "        # Expand c to [B, 1, D] to broadcast across K negatives\n",
    "        neg_score = (n * c.unsqueeze(1)).sum(dim=-1)  # [B, K]\n",
    "\n",
    "        return pos_score, neg_score\n",
    "\n",
    "    def sgns_loss(self, pos_score, neg_score):\n",
    "        # -log Ïƒ(pos) - sum log Ïƒ(-neg)\n",
    "        loss_pos = F.logsigmoid(pos_score)              # [B]\n",
    "        loss_neg = F.logsigmoid(-neg_score).sum(dim=1)  # [B]\n",
    "        loss = -(loss_pos + loss_neg).mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        centers, positives, negatives = batch   # shapes: [B], [B], [B,K]\n",
    "        pos_score, neg_score = self(centers, positives, negatives)\n",
    "        loss = self.sgns_loss(pos_score, neg_score)\n",
    "\n",
    "        # (Optional) A couple of easy diagnostics to ensure things are sane\n",
    "        with torch.no_grad():\n",
    "            # Probability-like metrics just for logging intuition\n",
    "            pos_prob = torch.sigmoid(pos_score).mean()\n",
    "            neg_prob = torch.sigmoid(neg_score).mean()\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True,\n",
    "                 on_epoch=True)\n",
    "        self.log(\"train/pos_prob\", pos_prob, prog_bar=False, on_step=True,\n",
    "                 on_epoch=True)\n",
    "        self.log(\"train/neg_prob\", neg_prob, prog_bar=False, on_step=True,\n",
    "                 on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam is perfectly fine here (sparse updates are optional)\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d83b4be-5c7a-4b44-8872-3fd421b6e38d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:28.478008Z",
     "iopub.status.busy": "2025-10-24T06:08:28.477802Z",
     "iopub.status.idle": "2025-10-24T06:08:28.981183Z",
     "shell.execute_reply": "2025-10-24T06:08:28.980642Z",
     "shell.execute_reply.started": "2025-10-24T06:08:28.477989Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(itos)\n",
    "model = SkipGramNegativeSampling(vocab_size=vocab_size, dim=300, lr=2.5e-3,\n",
    "                                 num_negatives=5)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"32-true\",   # \"16-mixed\" works too if you like\n",
    "    log_every_n_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e430e7c0-a9e8-4981-a02b-d29d36049fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T06:08:33.950256Z",
     "iopub.status.busy": "2025-10-24T06:08:33.949993Z",
     "iopub.status.idle": "2025-10-24T06:08:55.408735Z",
     "shell.execute_reply": "2025-10-24T06:08:55.407325Z",
     "shell.execute_reply.started": "2025-10-24T06:08:33.950235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 06:08:34.739581: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-24 06:08:34.753756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761286114.771977    3889 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761286114.777655    3889 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-24 06:08:34.795727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | in_embed  | Embedding | 21.4 M | train\n",
      "1 | out_embed | Embedding | 21.4 M | train\n",
      "------------------------------------------------\n",
      "42.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "42.8 M    Total params\n",
      "171.096   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d0e3d65e4245d3aee5734c45aa9bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db943c58-bcc7-4158-b8ce-2e38d8790927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68c3aa-4c05-4ec4-81ea-29ca99831fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35ee08-fea6-4f46-b186-a71f51915ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
