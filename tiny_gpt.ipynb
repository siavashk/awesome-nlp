{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89c913fa-3378-4695-8b8a-8b7ed774ebfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:39:19.639622Z",
     "iopub.status.busy": "2025-10-27T19:39:19.639218Z",
     "iopub.status.idle": "2025-10-27T19:39:19.644565Z",
     "shell.execute_reply": "2025-10-27T19:39:19.643825Z",
     "shell.execute_reply.started": "2025-10-27T19:39:19.639591Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lightning.pytorch import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de592f8b-d8d1-4936-b8cf-14f26d1778a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:11:05.929463Z",
     "iopub.status.busy": "2025-10-27T19:11:05.929078Z",
     "iopub.status.idle": "2025-10-27T19:11:06.285472Z",
     "shell.execute_reply": "2025-10-27T19:11:06.284533Z",
     "shell.execute_reply.started": "2025-10-27T19:11:05.929430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tiny Shakespeare dataset (1115394 characters)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "with open(\"../data/tiny_shakespeare.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(f\"Saved tiny Shakespeare dataset ({len(text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5359f6cc-19a8-4614-b68e-61fc2e01ef05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:19:13.578326Z",
     "iopub.status.busy": "2025-10-27T19:19:13.578036Z",
     "iopub.status.idle": "2025-10-27T19:19:13.784105Z",
     "shell.execute_reply": "2025-10-27T19:19:13.783262Z",
     "shell.execute_reply.started": "2025-10-27T19:19:13.578303Z"
    }
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "data_ids = encode(text)\n",
    "split = int(0.9 * len(data_ids))\n",
    "TRAIN_IDS, VAL_IDS = data_ids[:split], data_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28404b41-78ca-4691-a960-08a2b3c9a371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:25:35.189092Z",
     "iopub.status.busy": "2025-10-27T19:25:35.188784Z",
     "iopub.status.idle": "2025-10-27T19:25:35.195541Z",
     "shell.execute_reply": "2025-10-27T19:25:35.194595Z",
     "shell.execute_reply.started": "2025-10-27T19:25:35.189060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1337\n"
     ]
    }
   ],
   "source": [
    "seed_everything(1337)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# data / model hyperparams\n",
    "BLOCK_SIZE = 128     # context length\n",
    "N_LAYERS = 4\n",
    "N_HEADS = 4\n",
    "N_EMBED = 256     # must be divisible by N_HEADS\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# training hyperparams\n",
    "BATCH_SIZE = 64\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_STEPS = 800  # bump for better quality\n",
    "WARMUP_STEPS = 50\n",
    "EVAL_EVERY_N = 100\n",
    "SAMPLE_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b45436fb-062a-43f4-b96b-b3c4e93e5644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:19:14.520503Z",
     "iopub.status.busy": "2025-10-27T19:19:14.520203Z",
     "iopub.status.idle": "2025-10-27T19:19:14.524941Z",
     "shell.execute_reply": "2025-10-27T19:19:14.524045Z",
     "shell.execute_reply.started": "2025-10-27T19:19:14.520481Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(s: str) -> torch.Tensor:\n",
    "    return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "\n",
    "\n",
    "def decode(t: torch.Tensor) -> str:\n",
    "    return \"\".join(itos[int(i)] for i in t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83d00731-68fd-4add-a541-0685cc8192f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:39:02.267869Z",
     "iopub.status.busy": "2025-10-27T19:39:02.266824Z",
     "iopub.status.idle": "2025-10-27T19:39:02.274239Z",
     "shell.execute_reply": "2025-10-27T19:39:02.273003Z",
     "shell.execute_reply.started": "2025-10-27T19:39:02.267832Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(ids: torch.Tensor, batch_size: int,\n",
    "              block_size: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    ix = torch.randint(0, len(ids) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([ids[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([ids[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1148c8bb-1af3-4606-bb8c-ae3e07be53d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:20:59.632472Z",
     "iopub.status.busy": "2025-10-27T19:20:59.632196Z",
     "iopub.status.idle": "2025-10-27T19:20:59.639322Z",
     "shell.execute_reply": "2025-10-27T19:20:59.638387Z",
     "shell.execute_reply.started": "2025-10-27T19:20:59.632450Z"
    }
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head masked self-attention implemented from scratch.\"\"\"\n",
    "    def __init__(self, n_embed: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embed // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.k_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.v_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.o_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # precompute max-length causal mask\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(BLOCK_SIZE, BLOCK_SIZE), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        H, D = self.n_heads, self.head_dim\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, H, D).transpose(1, 2)  # (B,H,T,D)\n",
    "        k = self.k_proj(x).view(B, T, H, D).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, H, D).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(D)       # (B,H,T,T)\n",
    "        att = att.masked_fill(self.mask[:T, :T], float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v                                          # (B,H,T,D)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, H*D)   # (B,T,C)\n",
    "        y = self.resid_drop(self.o_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66cbb247-80d0-45a3-ae1e-861dc11a11db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:21:28.955958Z",
     "iopub.status.busy": "2025-10-27T19:21:28.955691Z",
     "iopub.status.idle": "2025-10-27T19:21:28.959666Z",
     "shell.execute_reply": "2025-10-27T19:21:28.958895Z",
     "shell.execute_reply.started": "2025-10-27T19:21:28.955939Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embed: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a9a42e-5a4f-4dbe-b16e-a44ec910b2be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:21:50.736640Z",
     "iopub.status.busy": "2025-10-27T19:21:50.736354Z",
     "iopub.status.idle": "2025-10-27T19:21:50.741191Z",
     "shell.execute_reply": "2025-10-27T19:21:50.740377Z",
     "shell.execute_reply.started": "2025-10-27T19:21:50.736621Z"
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.attn = CausalSelfAttention(n_embed, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.mlp = MLP(n_embed, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68180f8-2a14-4100-b5c5-dce117575f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:25:39.199147Z",
     "iopub.status.busy": "2025-10-27T19:25:39.198673Z",
     "iopub.status.idle": "2025-10-27T19:25:39.215591Z",
     "shell.execute_reply": "2025-10-27T19:25:39.214708Z",
     "shell.execute_reply.started": "2025-10-27T19:25:39.199125Z"
    }
   },
   "outputs": [],
   "source": [
    "class TinyGPT(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_embed: int = N_EMBED,\n",
    "        n_layers: int = N_LAYERS,\n",
    "        n_heads: int = N_HEADS,\n",
    "        dropout: float = DROPOUT,\n",
    "        block_size: int = BLOCK_SIZE,\n",
    "        lr: float = LR,\n",
    "        weight_decay: float = WEIGHT_DECAY,\n",
    "        warmup_steps: int = WARMUP_STEPS,\n",
    "        sample_len: int = SAMPLE_LEN,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embed)  # <-- learned positional encoding\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(n_embed, n_heads, dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.token_emb.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "\n",
    "        # simple warmup + cosine decay scheduler tied to max_steps (set in Trainer)\n",
    "        def lr_lambda(step):\n",
    "            if step < self.hparams.warmup_steps:\n",
    "                return max(1e-8, step / max(1, self.hparams.warmup_steps))\n",
    "            # cosine to zero over remaining steps\n",
    "            total = max(1, self.trainer.max_steps - self.hparams.warmup_steps)\n",
    "            step2 = min(step - self.hparams.warmup_steps, total)\n",
    "            return 0.5 * (1 + math.cos(math.pi * step2 / total))\n",
    "        sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"step\"}}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        _, loss = self(x, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        _, loss = self(x, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        # print a sample when training finishes\n",
    "        with torch.no_grad():\n",
    "            start = torch.randint(low=0, high=self.hparams.vocab_size, size=(1,1), device=self.device)\n",
    "            sample = self.generate(start, max_new_tokens=self.hparams.sample_len, temperature=1.0, top_k=0)[0].cpu()\n",
    "            print(\"\\n=== SAMPLE ===\\n\")\n",
    "            print(decode(sample))\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None):\n",
    "        B, T = idx.shape\n",
    "        tok = self.token_emb(idx)                                # (B,T,C)\n",
    "        pos_ids = torch.arange(T, device=idx.device)\n",
    "        pos = self.pos_emb(pos_ids)[None, :, :].expand(B, T, -1) # (B,T,C)\n",
    "        x = self.drop(tok + pos)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)                                 # (B,T,V)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: int = 0):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.hparams.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "            if top_k > 0:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                thresh = v[:, [-1]]\n",
    "                logits = torch.where(logits < thresh, torch.full_like(logits, -float(\"inf\")), logits)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8430e340-ecca-481a-9592-4608badc7794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:36:52.952036Z",
     "iopub.status.busy": "2025-10-27T19:36:52.951599Z",
     "iopub.status.idle": "2025-10-27T19:36:52.957590Z",
     "shell.execute_reply": "2025-10-27T19:36:52.956650Z",
     "shell.execute_reply.started": "2025-10-27T19:36:52.952009Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharDataModule:\n",
    "    \"\"\"Tiny helper (not a LightningDataModule to keep dependencies minimal).\"\"\"\n",
    "    def __init__(self, train_ids, val_ids, batch_size=BATCH_SIZE, block_size=BLOCK_SIZE):\n",
    "        self.train_ids = train_ids\n",
    "        self.val_ids = val_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # use a generator-like iterable to avoid holding big tensors on GPU\n",
    "        def collate(_):\n",
    "            x, y = get_batch(self.train_ids, self.batch_size, self.block_size)\n",
    "            return x, y\n",
    "        return torch.utils.data.DataLoader([0]*10_000_000, batch_size=None, collate_fn=collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        def collate(_):\n",
    "            x, y = get_batch(self.val_ids, self.batch_size, self.block_size)\n",
    "            return x, y\n",
    "        return torch.utils.data.DataLoader([0]*10_000, batch_size=None, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "502f060f-3a47-4ea9-8da1-c2447cc950f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:37:21.256544Z",
     "iopub.status.busy": "2025-10-27T19:37:21.256247Z",
     "iopub.status.idle": "2025-10-27T19:37:21.426498Z",
     "shell.execute_reply": "2025-10-27T19:37:21.425819Z",
     "shell.execute_reply.started": "2025-10-27T19:37:21.256510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "dm = CharDataModule(TRAIN_IDS, VAL_IDS, BATCH_SIZE, BLOCK_SIZE)\n",
    "\n",
    "model = TinyGPT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_embed=N_EMBED,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    sample_len=SAMPLE_LEN,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\", filename=\"tinygpt-{step:06d}-{val_loss:.3f}\")\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_steps=MAX_STEPS,\n",
    "    val_check_interval=EVAL_EVERY_N,   # run validation every N steps\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=callbacks,\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\" if torch.cuda.is_available() else \"32-true\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daf279e6-21ac-4421-bde2-35bea9564d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:39:24.628624Z",
     "iopub.status.busy": "2025-10-27T19:39:24.628303Z",
     "iopub.status.idle": "2025-10-27T19:49:06.138344Z",
     "shell.execute_reply": "2025-10-27T19:49:06.137182Z",
     "shell.execute_reply.started": "2025-10-27T19:39:24.628603Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type       | Params | Mode\n",
      "------------------------------------------------\n",
      "0 | token_emb | Embedding  | 16.6 K | eval\n",
      "1 | pos_emb   | Embedding  | 32.8 K | eval\n",
      "2 | drop      | Dropout    | 0      | eval\n",
      "3 | blocks    | ModuleList | 3.2 M  | eval\n",
      "4 | ln_f      | LayerNorm  | 512    | eval\n",
      "5 | lm_head   | Linear     | 16.6 K | eval\n",
      "------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.819    Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "70        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad12d27f9d34d6588d020dcf98e91bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81574a01050463c80dfdbdd153e9622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092910006a1a414da950783063e28344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc88d25-a3a2-4990-80dd-abdebb05494f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
